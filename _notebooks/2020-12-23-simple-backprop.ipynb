{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bare-Bones Backpropagation\n",
    "> Demonstrating the simplest possible backpropagation implementation, with all the clutter removed.\n",
    "\n",
    "- toc:true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Charlie Blake\n",
    "- categories: [neural-networks, backpropagation]\n",
    "- image: images/blog/simple-backprop/viz.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Backprop Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few times I came across backpropagation I struggled to get a feel for what was going on. It's not just enough\n",
    "to follow the equations - I couldn't visualise the operations and updates, especially the mysterious backwards pass. \n",
    "\n",
    "If someone had shown me then how simple it was to implement the key part of the training algorithm that figures out how to update the weights, then it would have helped me a lot. I understood how the chain rule worked and its relevance here, but I didn't have a picture of it in my head. I got bogged down in the matrix notation and PyTorch tensors and lost sight of what was really going on.\n",
    "\n",
    "So this is a simple-as-possible backprop implementation, to clear up that confusion. I don't go into the maths; I assume the reader already knows what's going on in theory, but doesn't have a great feel for what happens in practice.\n",
    "\n",
    "This can also serve as a reference for how to implement this from scratch a clean way. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, there's some setup to do. This isn't a tutorial on data loading, so I'm just going to paste some\n",
    "code for loading up our dataset and we can ignore the details. The only thing worth noting is that we'll be using the\n",
    "classic *MNIST* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from functools import reduce\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "batch_sz = 64\n",
    "train = DataLoader(MNIST('data/', train=True, download=True,\n",
    "                         transform=torchvision.transforms.Compose([\n",
    "                             torchvision.transforms.ToTensor(),\n",
    "                             lambda x: torch.flatten(x),\n",
    "                         ])\n",
    "                        ), batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `train` dataloader can be iterated over and returns minibatches of shape `(batch__sz, input_dim)`.\n",
    "\n",
    "In our case, these values are `(64, 28*28=784)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-by-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll construct our neural network by making classes for each layer. We'll use a standard setup of: linear layer, ReLU, linear layer, softmax; plus a cross-entropy loss.\n",
    "\n",
    "For each layer class we require two methods:\n",
    "\n",
    "- **`__call__(self, x)`**: implements the forward pass. `__call__` allows us to feed an input through the layer by treating the initialised layer object as a function. For example: `relu_layer = ReLU(); output = relu_layer(input)`.\n",
    "\n",
    "\n",
    "- **`bp_input(self, grad)`**: implements the backward pass, allowing us to backpropagate the gradient vector through the layer. The `grad` parameter is a matrix of partial derivatives of the loss, with respect to the data sent from the given layer to the next. As such, it is a `(batch_sz, out)` matrix. The job of `bp_input` is to return a `(batch_sz, in)` matrix to be sent to the next layer by multiplying `grad` by the derivative of the forward pass with respect to the _input_ (or an equivalent operation).\n",
    "\n",
    "There are two other methods we sometimes wish to implement for different layers:\n",
    "\n",
    "- **`__init__(self, ...)`**: initialises the layer, e.g. weights.\n",
    "\n",
    "\n",
    "- **`bp_param(self, grad)`**: the \"final stop\" of the backwards pass. Only applicable for layers with trainable weights. Similar to `bp_input`, but calculates the derivative with respect to the _weights_ of the layer. Should return a matrix with the same shape as the weights (`self.W`) to be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: The key point to recall when visualising this is that when we have a batch dimension it is always the first dimension. For both the forward and backward pass. This makes everything much simpler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "Let's start with the linear layer. We do the following:\n",
    "1. We start by initialising the weights (in this case using the Xavier initialisation).\n",
    "2. We then implement the call method. Rather than adding an explicit bias, we append a vector of ones to the layer's input (this is equivalent, and makes backprop simpler).\n",
    "3. Backpropagation with respect to the input is just right multiplication by the transpose of the weight matrix (adjusted to remove the added 1s column)\n",
    "4. Backpropagation with respect to the output is left multiplication by the transpose of the input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, in_sz, out_sz): self.W = self._xavier_init(in_sz + 1, out_sz)  # (in+1, out)\n",
    "    def _xavier_init(self, i, o): return torch.Tensor(i, o).uniform_(-1, 1) * math.sqrt(6./(i + o))\n",
    "    \n",
    "    def __call__(self, X):  # (batch_sz, in)\n",
    "        self.X = torch.cat([X, torch.ones(X.shape[0], 1)], dim=1)  # (batch_sz, in+1)\n",
    "        return self.X @ self.W  # (batch_sz, in+1) @ (in+1, out) = (batch_sz, out)\n",
    "    \n",
    "    def bp_input(self, grad): return (grad @ self.W.T)[:,:-1]  # (batch_sz, out) @ (out, in) =  (batch_sz, in) \n",
    "    def bp_param(self, grad): return self.X.T @ grad  # (in+1, batch_sz) @ (batch_sz, out) = (in+1, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Layer\n",
    "\n",
    "Some non-linearity is a must! Bring on the RelU function. The implementation is pretty obvious here. `clamp()` is doing all the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __call__(self, X):\n",
    "        self.X = X\n",
    "        return X.clamp(min=0)  # (batch_sz, in)\n",
    "    \n",
    "    def bp_input(self, grad): return grad * (self.X > 0).float()  # (batch_sz, in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax & Cross Entropy Loss\n",
    "\n",
    "What? Both at once, why would you do this??\n",
    "\n",
    "This is quite common, and I can justify it in two ways:\n",
    "\n",
    "1. This layer-loss combination often go together, so why not put them all in one layer? This saves us from having to do two separate forward and backward propagation steps.\n",
    "2. I won't prove it here, but it turns out that the derivative of the loss with respect to the input to the softmax, is much simpler than the two intermediate derivative operations, and bypasses the numerical stability issues that arise when we do the exponential and the logarithm. Phew!\n",
    "\n",
    "The downside here is that is we're just doing _inference_ then we only want the softmax output. But for the purposes of this tutorial we only really care about training. So this will do just fine!\n",
    "\n",
    "There's a trick in the second line of the softmax implementation: it turns out subtracting the argmax from the softmax input keeps the output the same, but the intermediate values are more numerically stable. How neat!\n",
    "\n",
    "Finally, we examine the backprop step. It's so simple! Our starting grad for backprop (the initial `grad` value passed in is just the ones vector) is the difference in our predicted output vector and the actual one-hot encoded label. This is so intuitive and wonderful.\n",
    "\n",
    "> Tip: This is exactly the same derivative as when we don't use a softmax layer and apply an MSE loss (i.e. the regression case). We can thus think of softmax + cross entropy as a way of getting to the same underlying backprop, but in the classification case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLoss:  # (batch_sz, in=out) for all dims in this layer\n",
    "    def __call__(self, X, Y):\n",
    "        self.Y = Y\n",
    "        self.Y_prob = self._softmax(X)\n",
    "        self.loss = self._cross_entropy_loss(Y, self.Y_prob)\n",
    "        return self.Y_prob, self.loss\n",
    "    \n",
    "    def _softmax(self, X):\n",
    "        self.X = X\n",
    "        X_adj = X - X.amax(dim=1, keepdim=True)\n",
    "        exps = torch.exp(X_adj)\n",
    "        return exps / exps.sum(axis=1, keepdim=True)\n",
    "    \n",
    "    def _cross_entropy_loss(self, Y, Y_prob): return (-Y * torch.log(Y_prob)).sum(axis=1).mean()\n",
    "    \n",
    "    def bp_input(self, grad): return (self.Y_prob - self.Y) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's bring these layers together in a class: our `NeuralNet` implementation.\n",
    "\n",
    "The `evaluate()` function does two things. Firstly, it runs the forward pass by chaining the `__call__()` functions, to generate label probabilities. Secondly, it uses the labels passed to it to calculate the loss and percentage correctly predicted.\n",
    "\n",
    "> Note: for this simplified example we don't have a pure inference function, but we could add one with a small change to `SoftmaxCrossEntropyLoss`.\n",
    "\n",
    "The `gradient_descent()` function then gets the matrix of updates for each weight matrix and applies the update. The key bit here is how `backprop()` works. Going backwards through the computation graph we chain the backprop with respect to input methods. Then for each weighted layer we want to update, we apply the backprop with respect to prameters method to the relevant gradient vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, input_size=28*28, hidden_size=32, output_size=10, alpha=0.001):\n",
    "        self.alpha = alpha\n",
    "        self.z1 = LinearLayer(input_size, hidden_size)\n",
    "        self.a1 = ReLU()\n",
    "        self.z2 = LinearLayer(hidden_size, output_size)\n",
    "        self.loss = SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        out = self.z2(self.a1(self.z1(X)))\n",
    "        correct = torch.eq(out.argmax(axis=1), Y).double().mean()\n",
    "        Y_prob, loss = self.loss(out, one_hot(Y, 10))\n",
    "        return Y_prob, correct, loss\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        delta_W1, delta_W2 = self.backprop()\n",
    "        self.z1.W -= self.alpha * delta_W1\n",
    "        self.z2.W -= self.alpha * delta_W2\n",
    "    \n",
    "    def backprop(self):\n",
    "        d_out = torch.ones(*self.loss.Y.shape)\n",
    "        d_z2 = self.loss.bp_input(d_out)\n",
    "        d_a1 = self.z2.bp_input(d_z2)\n",
    "        d_z1 = self.a1.bp_input(d_a1)\n",
    "\n",
    "        d_w2 = self.z2.bp_param(d_z2)\n",
    "        d_w1 = self.z1.bp_param(d_z1)\n",
    "        return d_w1, d_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We're almost there! I won't go into this bit too much because this tutorial isn't about training loops, but it's all very standard here.\n",
    "\n",
    "We break the training data into minibatches and train on them over 10 epochs. The evaluation metrics plotted are those recorded during regular training.\n",
    "\n",
    "> Warning: these results are only on the training set! In practice we should *always* plot performance on a test set, but we don't want to clutter the tutorial with this extra detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "model = NeuralNet()\n",
    "stats = {'correct': [], 'loss': [], 'epoch': []}\n",
    "for epoch in range(2):\n",
    "    correct, loss = 0, 0\n",
    "    for i, (X, y) in enumerate(train):\n",
    "        y_prob, batch_correct, batch_loss = model.evaluate(X, y)\n",
    "        model.gradient_descent()\n",
    "        \n",
    "        correct += batch_correct / len(train)\n",
    "        loss += batch_loss / len(train)\n",
    "    stats['correct'].append(correct.item())\n",
    "    stats['loss'].append(loss.item())\n",
    "    stats['epoch'].append(epoch)\n",
    "    print(f'epoch: {epoch} | correct: {correct:.2f}, loss: {loss:.2f}')\n",
    "\n",
    "base = alt.Chart(pd.DataFrame.from_dict(stats)).mark_line() \\\n",
    "          .encode(alt.X('epoch', axis=alt.Axis(title='epoch')))\n",
    "line1 = base.mark_line(stroke='#5276A7', interpolate='monotone') \\\n",
    "            .encode(alt.Y('loss'   , axis=alt.Axis(title='Loss'   , titleColor='#5276A7'), scale=alt.Scale(domain=[0.0, max(stats['loss'   ])])), tooltip='loss'   )\n",
    "line2 = base.mark_line(stroke='#57A44C', interpolate='monotone') \\\n",
    "            .encode(alt.Y('correct', axis=alt.Axis(title='Correct', titleColor='#57A44C'), scale=alt.Scale(domain=[min(stats['correct']), 1.0])), tooltip='correct')\n",
    "alt.layer(line1, line2).resolve_scale(y = 'independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results are great! After 10 epochs I'm getting a whopping 97% correct. \n",
    "\n",
    "And we've implemented this all from scratch, backprop included, using only 4 notebook cells worth of code. Hopefully this reflects how simple the underlying implementation really is!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xeus-ml]",
   "language": "python",
   "name": "conda-env-xeus-ml-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
