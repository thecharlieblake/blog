<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Bare-Bones Backpropagation | Charlie Blake: Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Bare-Bones Backpropagation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Charlie Blake’s personal blog" />
<meta property="og:description" content="Charlie Blake’s personal blog" />
<link rel="canonical" href="https://thecharlieblake.co.uk/2020/12/23/simple-backprop.html" />
<meta property="og:url" content="https://thecharlieblake.co.uk/2020/12/23/simple-backprop.html" />
<meta property="og:site_name" content="Charlie Blake: Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-23T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://thecharlieblake.co.uk/2020/12/23/simple-backprop.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thecharlieblake.co.uk/2020/12/23/simple-backprop.html"},"headline":"Bare-Bones Backpropagation","dateModified":"2020-12-23T00:00:00-06:00","datePublished":"2020-12-23T00:00:00-06:00","description":"Charlie Blake’s personal blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thecharlieblake.co.uk/feed.xml" title="Charlie Blake: Blog" /><link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Charlie Blake: Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bare-Bones Backpropagation</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-23T00:00:00-06:00" itemprop="datePublished">
        Dec 23, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/thecharlieblake/blog/tree/master/_notebooks/2020-12-23-simple-backprop.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/thecharlieblake/blog/master?filepath=_notebooks%2F2020-12-23-simple-backprop.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/thecharlieblake/blog/blob/master/_notebooks/2020-12-23-simple-backprop.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Making-Backprop-Simple">Making Backprop Simple </a></li>
<li class="toc-entry toc-h2"><a href="#Setup">Setup </a></li>
<li class="toc-entry toc-h2"><a href="#Layer-by-layer">Layer-by-layer </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Linear-Layer">Linear Layer </a></li>
<li class="toc-entry toc-h3"><a href="#ReLU-Layer">ReLU Layer </a></li>
<li class="toc-entry toc-h3"><a href="#Softmax-&-Cross-Entropy-Loss">Softmax &amp; Cross Entropy Loss </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Putting-it-all-together">Putting it all together </a></li>
<li class="toc-entry toc-h2"><a href="#Training-the-model">Training the model </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-23-simple-backprop.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Making-Backprop-Simple">
<a class="anchor" href="#Making-Backprop-Simple" aria-hidden="true"><span class="octicon octicon-link"></span></a>Making Backprop Simple<a class="anchor-link" href="#Making-Backprop-Simple"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first few times I came across backpropagation I struggled to get a feel for what was going on. It's not just enough
to follow the equations - I couldn't visualise the operations and updates in a nice, clean way.</p>
<p>If someone had shown me then how it's really just a generalisation of our old friend the chain rule--and barely any
more complex--then I think it would have helped me a lot. But instead I got bogged down in the matrix notation, and
figuring out what dimension went with what, and lost sight of what was really going on.</p>
<p>So this is a simple-as-possible backprop implementation. I don't go into the maths here. I assume the reader already knows what's going on in theory, but doesn't have a great feel for what happens in practice, when we have batches and layers and thesors and so on.</p>
<p>This can also serve as a reference for how to implement this from scratch in the nicest way. Enjoy!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">
<a class="anchor" href="#Setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup<a class="anchor-link" href="#Setup"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First things first, there's some setup to do. This isn't a tutorial on data loading, so I'm just going to paste some
code for loading up our dataset and we can ignore the details. The only thing worth noting is that we'll be using the
classic <em>MNIST</em> dataset:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">one_hot</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">'data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                             <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                             <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                         <span class="p">])</span>
                        <span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_sz</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This <code>train</code> dataloader can be iterated over and returns minibatches of shape <code>(batch__sz, input_dim)</code>.</p>
<p>In our case, these values are <code>(64, 28*28=784)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Layer-by-layer">
<a class="anchor" href="#Layer-by-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layer-by-layer<a class="anchor-link" href="#Layer-by-layer"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll construct our neural network by making classes for each layer. We'll use a standard setup of: linear layer, ReLU, linear layer, softmax; plus a cross-entropy loss.</p>
<p>For each layer class we require two methods:</p>
<ul>
<li>
<strong><code>__call__(self, x)</code></strong>: implements the forward pass. <code>__call__</code> allows us to feed an input through the layer by treating the initialised layer object as a function. For example: <code>relu_layer = ReLU(); output = relu_layer(input)</code>.</li>
</ul>
<ul>
<li>
<strong><code>bp_input(self, grad)</code></strong>: implements the backward pass, allowing us to backpropagate the gradient vector through the layer. The <code>grad</code> parameter is a matrix of partial derivatives of the loss, with respect to the data sent from the given layer to the next. As such, it is a <code>(batch_sz, out)</code> matrix. The job of <code>bp_input</code> is to return a <code>(batch_sz, in)</code> matrix to be sent to the next layer by multiplying <code>grad</code> by the derivative of the forward pass with respect to the <em>input</em> (or an equivalent operation).</li>
</ul>
<p>There are two other methods we sometimes wish to implement for different layers:</p>
<ul>
<li>
<strong><code>__init__(self, ...)</code></strong>: initialises the layer, e.g. weights.</li>
</ul>
<ul>
<li>
<strong><code>bp_param(self, grad)</code></strong>: the "final stop" of the backwards pass. Only applicable for layers with trainable weights. Similar to <code>bp_input</code>, but calculates the derivative with respect to the <em>weights</em> of the layer. Should return a matrix with the same shape as the weights (<code>self.W</code>) to be updated.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>The key point to recall when visualising this is that when we have a batch dimension it is always the first dimension. For both the forward and backward pass. This makes everything much simpler!
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Linear-Layer">
<a class="anchor" href="#Linear-Layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Layer<a class="anchor-link" href="#Linear-Layer"> </a>
</h3>
<p>Let's start with the linear layer. We do the following:</p>
<ol>
<li>We start by initialising the weights (in this case using the Xavier initialisation).</li>
<li>We then implement the call method. Rather than adding an explicit bias, we append a vector of ones to the layer's input (this is equivalent, and makes backprop simpler).</li>
<li>Backpropagation with respect to the input is just right multiplication by the transpose of the weight matrix (adjusted to remove the added 1s column)</li>
<li>Backpropagation with respect to the output is left multiplication by the transpose of the input matrix.</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_sz</span><span class="p">,</span> <span class="n">out_sz</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_xavier_init</span><span class="p">(</span><span class="n">in_sz</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_sz</span><span class="p">)</span>  <span class="c1"># (in+1, out)</span>
    <span class="k">def</span> <span class="nf">_xavier_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span><span class="o">/</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">o</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>  <span class="c1"># (batch_sz, in)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_sz, in+1)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span>  <span class="c1"># (batch_sz, in+1) @ (in+1, out) = (batch_sz, out)</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">grad</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (batch_sz, out) @ (out, in) =  (batch_sz, in) </span>
    <span class="k">def</span> <span class="nf">bp_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">grad</span>  <span class="c1"># (in+1, batch_sz) @ (batch_sz, out) = (in+1, out)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ReLU-Layer">
<a class="anchor" href="#ReLU-Layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLU Layer<a class="anchor-link" href="#ReLU-Layer"> </a>
</h3>
<p>Some non-linearity is a must! Bring on the RelU function. The implementation is pretty obvious here. <code>clamp()</code> is doing all the work.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch_sz, in)</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># (batch_sz, in)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Softmax-&amp;-Cross-Entropy-Loss">
<a class="anchor" href="#Softmax-&amp;-Cross-Entropy-Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Softmax &amp; Cross Entropy Loss<a class="anchor-link" href="#Softmax-&amp;-Cross-Entropy-Loss"> </a>
</h3>
<p>What? Both at once, why would you do this??</p>
<p>This is quite common, and I can justify it in two ways:</p>
<ol>
<li>This layer-loss combination often go together, so why not put them all in one layer? This saves us from having to do two separate forward and backward propagation steps.</li>
<li>I won't prove it here, but it turns out that the derivative of the loss with respect to the input to the softmax, is much simpler than the two intermediate derivative operations, and bypasses the numerical stability issues that arise when we do the exponential and the logarithm. Phew!</li>
</ol>
<p>The downside here is that is we're just doing <em>inference</em> then we only want the softmax output. But for the purposes of this tutorial we only really care about training. So this will do just fine!</p>
<p>There's a trick in the second line of the softmax implementation: it turns out subtracting the argmax from the softmax input keeps the output the same, but the intermediate values are more numerically stable. How neat!</p>
<p>Finally, we examine the backprop step. It's so simple! Our starting grad for backprop (the initial <code>grad</code> value passed in is just the ones vector) is the difference in our predicted output vector and the actual one-hot encoded label. This is so intuitive and wonderful.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>This is exactly the same derivative as when we don’t use a softmax layer and apply an MSE loss (i.e. the regression case). We can thus think of softmax + cross entropy as a way of getting to the same underlying backprop, but in the classification case.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyLoss</span><span class="p">:</span>  <span class="c1"># (batch_sz, in=out) for all dims in this layer</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cross_entropy_loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">X_adj</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">exps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X_adj</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">exps</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Y_prob</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y_prob</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Putting-it-all-together">
<a class="anchor" href="#Putting-it-all-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting it all together<a class="anchor-link" href="#Putting-it-all-together"> </a>
</h2>
<p>Let's bring these layers together in a class: our <code>NeuralNet</code> implementation.</p>
<p>The <code>evaluate()</code> function does two things. Firstly, it runs the forward pass by chaining the <code>__call__()</code> functions, to generate label probabilities. Secondly, it uses the labels passed to it to calculate the loss and percentage correctly predicted.
</p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>for this simplified example we don’t have a pure inference function, but we could add one with a small change to <code>SoftmaxCrossEntropyLoss</code>.
</div>
The <code>gradient_descent()</code> function then gets the matrix of updates for each weight matrix and applies the update. The key bit here is how <code>backprop()</code> works. Going backwards through the computation graph we chain the backprop with respect to input methods. Then for each weighted layer we want to update, we apply the backprop with respect to prameters method to the relevant gradient vector.

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">Y_prob</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Y_prob</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">delta_W1</span><span class="p">,</span> <span class="n">delta_W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">delta_W1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">delta_W2</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">d_z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_out</span><span class="p">)</span>
        <span class="n">d_a1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_z2</span><span class="p">)</span>
        <span class="n">d_z1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_a1</span><span class="p">)</span>

        <span class="n">d_w2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">bp_param</span><span class="p">(</span><span class="n">d_z2</span><span class="p">)</span>
        <span class="n">d_w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="o">.</span><span class="n">bp_param</span><span class="p">(</span><span class="n">d_z1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">d_w1</span><span class="p">,</span> <span class="n">d_w2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-model">
<a class="anchor" href="#Training-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training the model<a class="anchor-link" href="#Training-the-model"> </a>
</h2>
<p>We're almost there! I won't go into this bit too much because this tutorial isn't about training loops, but it's all very standard here.</p>
<p>We break the training data into minibatches and train on them over 10 epochs. The evaluation metrics plotted are those recorded during regular training.
</p>
<div class="flash flash-error">
    <svg class="octicon octicon-alert" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z"></path></svg>
    <strong>Warning: </strong>these results are only on the training set! In practice we should <em>always</em> plot performance on a test set, but we don’t want to clutter the tutorial with this extra detail.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">()</span>
<span class="n">stats</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'correct'</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">'loss'</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">'epoch'</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train</span><span class="p">):</span>
        <span class="n">y_prob</span><span class="p">,</span> <span class="n">batch_correct</span><span class="p">,</span> <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">gradient_descent</span><span class="p">()</span>
        
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">batch_correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">batch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">'correct'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">'epoch'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1"> | correct: </span><span class="si">{</span><span class="n">correct</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">stats</span><span class="p">))</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span> \
          <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">)))</span>
<span class="n">line1</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">stroke</span><span class="o">=</span><span class="s1">'#5276A7'</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="s1">'monotone'</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">'loss'</span>   <span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">'Loss'</span>   <span class="p">,</span> <span class="n">titleColor</span><span class="o">=</span><span class="s1">'#5276A7'</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">'loss'</span>   <span class="p">])])),</span> <span class="n">tooltip</span><span class="o">=</span><span class="s1">'loss'</span>   <span class="p">)</span>
<span class="n">line2</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">stroke</span><span class="o">=</span><span class="s1">'#57A44C'</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="s1">'monotone'</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">'correct'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">'Correct'</span><span class="p">,</span> <span class="n">titleColor</span><span class="o">=</span><span class="s1">'#57A44C'</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">'correct'</span><span class="p">]),</span> <span class="mf">1.0</span><span class="p">])),</span> <span class="n">tooltip</span><span class="o">=</span><span class="s1">'correct'</span><span class="p">)</span>
<span class="n">alt</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">line1</span><span class="p">,</span> <span class="n">line2</span><span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="s1">'independent'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch: 0 | correct: 0.87, loss: 0.47
epoch: 1 | correct: 0.92, loss: 0.28
epoch: 2 | correct: 0.93, loss: 0.23
epoch: 3 | correct: 0.94, loss: 0.20
epoch: 4 | correct: 0.95, loss: 0.18
epoch: 5 | correct: 0.95, loss: 0.16
epoch: 6 | correct: 0.96, loss: 0.15
epoch: 7 | correct: 0.96, loss: 0.14
epoch: 8 | correct: 0.96, loss: 0.13
epoch: 9 | correct: 0.96, loss: 0.12
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-baa92da218ff4d3cb41d464cac84b660"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-baa92da218ff4d3cb41d464cac84b660") {
      outputDiv = document.getElementById("altair-viz-baa92da218ff4d3cb41d464cac84b660");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "layer": [{"mark": {"type": "line", "interpolate": "monotone", "stroke": "#5276A7"}, "encoding": {"tooltip": {"type": "quantitative", "field": "loss"}, "x": {"type": "quantitative", "axis": {"title": "epoch"}, "field": "epoch"}, "y": {"type": "quantitative", "axis": {"title": "Loss", "titleColor": "#5276A7"}, "field": "loss", "scale": {"domain": [0.0, 0.47194650769233704]}}}}, {"mark": {"type": "line", "interpolate": "monotone", "stroke": "#57A44C"}, "encoding": {"tooltip": {"type": "quantitative", "field": "correct"}, "x": {"type": "quantitative", "axis": {"title": "epoch"}, "field": "epoch"}, "y": {"type": "quantitative", "axis": {"title": "Correct", "titleColor": "#57A44C"}, "field": "correct", "scale": {"domain": [0.8700526385927538, 1.0]}}}}], "data": {"name": "data-551adda8aec31a1fe519546b549c9e76"}, "resolve": {"scale": {"y": "independent"}}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-551adda8aec31a1fe519546b549c9e76": [{"correct": 0.8700526385927538, "loss": 0.47194650769233704, "epoch": 0}, {"correct": 0.9205423773987237, "loss": 0.2753719389438629, "epoch": 1}, {"correct": 0.9332855810234594, "loss": 0.22961291670799255, "epoch": 2}, {"correct": 0.9432469349680197, "loss": 0.19969312846660614, "epoch": 3}, {"correct": 0.9497101545842259, "loss": 0.17908376455307007, "epoch": 4}, {"correct": 0.953991204690836, "loss": 0.16318272054195404, "epoch": 5}, {"correct": 0.9573727345415821, "loss": 0.15032540261745453, "epoch": 6}, {"correct": 0.960104610874205, "loss": 0.13999834656715393, "epoch": 7}, {"correct": 0.9624700159914752, "loss": 0.1315470188856125, "epoch": 8}, {"correct": 0.9643356876332664, "loss": 0.12460409104824066, "epoch": 9}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And our results are great! After 10 epochs I'm getting a whopping 97% correct.</p>
<p>Given we've implemented this all from scratch, backprop included, to get these results using only 4 notebook cells worth of code is a testament to how simple backprop really is!</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/2020/12/23/simple-backprop.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Charlie Blake&#39;s personal blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
