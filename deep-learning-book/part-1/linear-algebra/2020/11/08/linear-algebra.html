<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning Book Notes: Linear Algebra | Charlie Blake: Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Deep Learning Book Notes: Linear Algebra" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes for Chapter 2 in the book Deep Learning (Goodfellow et al.)." />
<meta property="og:description" content="Notes for Chapter 2 in the book Deep Learning (Goodfellow et al.)." />
<link rel="canonical" href="https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html" />
<meta property="og:url" content="https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html" />
<meta property="og:site_name" content="Charlie Blake: Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-08T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Notes for Chapter 2 in the book Deep Learning (Goodfellow et al.).","mainEntityOfPage":{"@type":"WebPage","@id":"https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html"},"url":"https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html","@type":"BlogPosting","headline":"Deep Learning Book Notes: Linear Algebra","dateModified":"2020-11-08T00:00:00-06:00","datePublished":"2020-11-08T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thecharlieblake.co.uk/feed.xml" title="Charlie Blake: Blog" /><link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Charlie Blake: Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Learning Book Notes: Linear Algebra</h1><p class="page-description">Notes for Chapter 2 in the book Deep Learning (Goodfellow et al.).</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-08T00:00:00-06:00" itemprop="datePublished">
        Nov 8, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#deep-learning-book">deep-learning-book</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#part-1">part-1</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#linear-algebra">linear-algebra</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#matrix-multiplication">Matrix Multiplication</a>
<ul>
<li class="toc-entry toc-h3"><a href="#key-terms">Key Terms</a></li>
<li class="toc-entry toc-h3"><a href="#motivation">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#solutions">Solutions</a></li>
<li class="toc-entry toc-h3"><a href="#inverse-matrices">Inverse Matrices</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#norms">Norms</a>
<ul>
<li class="toc-entry toc-h3"><a href="#motivation-1">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#definition">Definition</a></li>
<li class="toc-entry toc-h3"><a href="#vector-norms">Vector Norms</a></li>
<li class="toc-entry toc-h3"><a href="#matrix-norms">Matrix Norms</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#eigendecomposition">Eigendecomposition</a>
<ul>
<li class="toc-entry toc-h3"><a href="#key-terms-1">Key Terms</a></li>
<li class="toc-entry toc-h3"><a href="#motivation-2">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#eigenvectors--eigenvalues">Eigenvectors &amp; Eigenvalues</a></li>
<li class="toc-entry toc-h3"><a href="#the-determinant">The Determinant</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#singular-value-decomposition">Singular Value Decomposition</a>
<ul>
<li class="toc-entry toc-h3"><a href="#motivation-3">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#singular-values--vectors">Singular Values &amp; Vectors</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#pseudoinverse">Pseudoinverse</a>
<ul>
<li class="toc-entry toc-h3"><a href="#motivation-4">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#the-moore-penrose-pseudoinverse">The Moore-Penrose Pseudoinverse</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#principal-component-analysis">Principal Component Analysis</a>
<ul>
<li class="toc-entry toc-h3"><a href="#motivation-5">Motivation</a></li>
<li class="toc-entry toc-h3"><a href="#process">Process</a></li>
<li class="toc-entry toc-h3"><a href="#choice-of-decoding-matrix">Choice of Decoding Matrix</a></li>
</ul>
</li>
</ul><p>Link to chapter: <a href="https://www.deeplearningbook.org/contents/linear_algebra.html">2 Linear Algebra</a></p>

<h2 id="matrix-multiplication">
<a class="anchor" href="#matrix-multiplication" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matrix Multiplication</h2>

<h3 id="key-terms">
<a class="anchor" href="#key-terms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key Terms</h3>

<p><strong>Span</strong> (of a matrix / set of vectors): the set of all vectors obtainable via linear combination</p>

<p><strong>Column / row space</strong>: span of a matrix’s columns / rows</p>

<p><strong>Linear independence</strong>: no vectors are in the span of any other group of vectors</p>

<p><strong>Invertible matrix</strong>: square matrix with linearly independent columns</p>

<h3 id="motivation">
<a class="anchor" href="#motivation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>Consider the “basic” matrix-vector multiplication equation: $Ax = b$, where $A$ is a fixed matrix and $x$ is a variable vector to be found.</p>

<p>For a fixed $b$, we care about the possible solutions (i.e. values of $x$): how many are there and what are they?</p>

<p>Considering the case where $b$ is arbitrary (i.e. considering what is true for <em>all</em> $b$) is perhaps more interesting, and can tell us a lot about $A$ and its properties. The key question is: does the equation have a solution for all $b$?</p>

<h3 id="solutions">
<a class="anchor" href="#solutions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Solutions</h3>

<p>In the case of a fixed $b$ the basic equation either has 0, 1 or $\infty$ many solutions. The authors provide a useful way of thinking about this:</p>

<blockquote>
  <p>To analyze how many solutions the equation has, think of the columns of $A$ as specifying diﬀerent directions we can travel in from the origin … then determine how many ways there are of reaching $b$.</p>
</blockquote>

<p>In the case of an arbitrary $b$:</p>

<ul>
  <li>There is <strong>&gt;= 1</strong> solution for all $b$ iff $A$ has <strong>a set</strong> of $m$ linearly independent columns.
    <ul>
      <li>This is due to the column space of $A$ being all of $\mathbb{R}^{m}$.</li>
      <li>A <em>necessary</em> (but not sufficient) condition here is $n \gt m$ (at least as many columns as rows), otherwise the column space can’t span $\mathbb{R}^{m}$.</li>
    </ul>
  </li>
  <li>There is <strong>= 1</strong> solution for all $b$ iff $A$ has <strong>exactly</strong> $m$ linearly independent columns.
    <ul>
      <li>A <em>necessary</em> condition is therefore that m = n (i.e. $A$ is square).</li>
      <li>If $A$ satisfies this condition we say it is <strong>invertible</strong>.</li>
      <li>Note that a square matrix that is <em>not</em> invertible is called <strong>singular</strong> or <strong>degenerate</strong>.</li>
    </ul>
  </li>
</ul>

<p>Why is this all useful? Consider…</p>

<h3 id="inverse-matrices">
<a class="anchor" href="#inverse-matrices" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inverse Matrices</h3>

<p>Think of $A$ as applying a transformation to a vector or matrix.</p>

<p>If the basic equation has one solution (i.e. $A$ is invertible) then this transformation can be reversed. This is often really useful!</p>

<p>This reversal can be expressed as the matrix inverse $A^{-1}$.</p>

<p>In practice computing $A^{-1}$ directly is often avoided as it can be numerically unstable, but this property is still very important.</p>

<h2 id="norms">
<a class="anchor" href="#norms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Norms</h2>

<h3 id="motivation-1">
<a class="anchor" href="#motivation-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>A norm is a function that gives us some <strong>measure of the distance from the origin</strong> to a vector or matrix.</p>

<p>Clearly this is a useful concept!</p>

<h3 id="definition">
<a class="anchor" href="#definition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Definition</h3>

<p>A norm is any function which satisfies the following three properties (for all $\alpha, x, y$):</p>

<p><strong>point-separating:</strong> $f(x) = 0 \implies x = 0$</p>

<p><strong>absolutely scalable:</strong> $f(\alpha x) = |\alpha|f(x)$</p>

<p><strong>triangle inequality:</strong> $f(x + y) &lt;= f(x) + f(y)$</p>

<h3 id="vector-norms">
<a class="anchor" href="#vector-norms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vector Norms</h3>

<p>The $L^p$ norm of $x$, often denoted by $||x||_p$ , is defined as:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mi>p</mi></msub><mo>=</mo><msup><mrow><mo fence="true">(</mo><munder><mo>∑</mo><mi>i</mi></munder><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><msup><mi mathvariant="normal">∣</mi><mi>p</mi></msup><mo fence="true">)</mo></mrow><mfrac><mn>1</mn><mi>p</mi></mfrac></msup><mspace width="1em"></mspace></mrow><annotation encoding="application/x-tex">||x||_p = \left(\sum_i |x_i|^p\right)^{\frac{1}{p}} \quad</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.3715889999999997em;vertical-align:-1.277669em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000003em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714392em;"><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:2.09392em;"><span style="top:-4.5029em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443142857142858em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.48288571428571425em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:1em;"></span></span></span></span></span>

<p>where ($p \in \mathbb{R}, p \ge 1$) .</p>

<p>The $L^1$ norm is called the <strong>Manhattan norm</strong>.</p>

<p>The $L^2$ norm is called the <strong>Euclidean norm</strong>. This is the standard norm and is commonly referred to without the subscript as simply $||x||$. The squared $L^2$ norm is also used in some contexts, which is simply $x^Tx$.</p>

<p>The $L^\infty$ norm is called the <strong>max norm</strong>. It is defined as $||x||_\infty = \max_i{|x_i|}$.</p>

<h3 id="matrix-norms">
<a class="anchor" href="#matrix-norms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matrix Norms</h3>

<p><em>(This is a much more <a href="https://en.wikipedia.org/wiki/Matrix_norm">complex</a> field that we only touch on briefly here!)</em></p>

<p>We consider two analogous matrix norms for the $L^2$ vector norm.</p>

<p>In wider mathematics the <strong>spectral norm</strong> is often used. It can be useful in ML for analysing (among other things) exploding/vanishing gradients. It is defined as $A$’s largest singular value: $||A|| = \sigma_{\max}{(A)}$</p>

<p>However, most in most ML applications it is assumed the <strong>Frobenius norm</strong> is used. This is defined as:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="normal">∥</mi><mi>A</mi><mi mathvariant="normal">∥</mi><msub><mi mathvariant="normal">∥</mi><mi>F</mi></msub><mo>=</mo><msqrt><mrow><munder><mo>∑</mo><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></munder><msubsup><mi>A</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|\|A\|\|_F = \sqrt{\sum_{i,j}{A^2_{i,j}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mord">∥</span><span class="mord mathdefault">A</span><span class="mord">∥</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">F</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.04em;vertical-align:-1.5880110000000003em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4519889999999998em;"><span class="svg-align" style="top:-5em;"><span class="pstrut" style="height:5em;"></span><span class="mord" style="padding-left:1em;"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8723309999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.795908em;"><span style="top:-2.4231360000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0448000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4129719999999999em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.4119889999999997em;"><span class="pstrut" style="height:5em;"></span><span class="hide-tail" style="min-width:1.02em;height:3.08em;"><svg width="400em" height="3.08em" viewbox="0 0 400000 3240" preserveaspectratio="xMinYMin slice"><path d="M473,2793
c339.3,-1799.3,509.3,-2700,510,-2702 l0 -0
c3.3,-7.3,9.3,-11,18,-11 H400000v40H1017.7
s-90.5,478,-276.2,1466c-185.7,988,-279.5,1483,-281.5,1485c-2,6,-10,9,-24,9
c-8,0,-12,-0.7,-12,-2c0,-1.3,-5.3,-32,-16,-92c-50.7,-293.3,-119.7,-693.3,-207,-1200
c0,-1.3,-5.3,8.7,-16,30c-10.7,21.3,-21.3,42.7,-32,64s-16,33,-16,33s-26,-26,-26,-26
s76,-153,76,-153s77,-151,77,-151c0.7,0.7,35.7,202,105,604c67.3,400.7,102,602.7,104,
606zM1001 80h400000v40H1017.7z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5880110000000003em;"><span></span></span></span></span></span></span></span></span></span>

<p>Note that this is equivalent to: $||A||_F = \sqrt{Tr(AA^T)}$</p>

<h2 id="eigendecomposition">
<a class="anchor" href="#eigendecomposition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Eigendecomposition</h2>

<h3 id="key-terms-1">
<a class="anchor" href="#key-terms-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key Terms</h3>

<p><strong>Unit vector</strong>: the $L^2$ norm = 1</p>

<p><strong>Orthogonal vectors</strong>: $x^Ty = 0$</p>

<p><strong>Orthonormal vectors</strong>: orthogonal unit vectors</p>

<p><strong>Orthogonal matrix</strong>: rows &amp; columns are mutually <em>orthonormal</em></p>

<h3 id="motivation-2">
<a class="anchor" href="#motivation-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>Decomposing matrices can help us learn about a matrix by breaking it down into its constituent parts. This can reveal useful properties about the matrix.</p>

<p>Eigendecomposition decomposes a matrix into <strong>eigenvectors</strong> and <strong>eigenvalues</strong>.</p>

<p>These tell us something about the directions and sizes of the transformation created when multiplying by the matrix.</p>

<h3 id="eigenvectors--eigenvalues">
<a class="anchor" href="#eigenvectors--eigenvalues" aria-hidden="true"><span class="octicon octicon-link"></span></a>Eigenvectors &amp; Eigenvalues</h3>

<p>Vector $v$ and scalar $\lambda$ are a eigenvector-eigenvalue pair for <em>square</em> matrix $A$ iff:</p>

<ol>
  <li>$v \neq \mathbf{0}$</li>
  <li>$Av = \lambda v$</li>
</ol>

<p><em>(Strictly speaking, here $v$ is a right eigenvector. A left eigenvector is such that $v^TA = \lambda v^T$. We care primarily about right eigenvectors.)</em></p>

<p>If $v$ is an eigenvector it follows that any rescaled version of $v$ is also an eigenvector with the same eigenvalue. We typically use a scale such that we have a unit eigenvector.</p>

<p>If $A$ has $n$ independent eigenvectors we can combine them into the <strong>columns</strong> of a matrix, $V$, such that $AV = V diag(\lambda)$.</p>

<p>The eigendecomposition of $A$ is then defined as: $A = V diag(\lambda) V^{-1}$.</p>

<p>We are only guaranteed an eigendecomposition if $A$ is symmetric (and real-valued). In this case it is often denoted:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Q</mi><mi mathvariant="normal">Λ</mi><msup><mi>Q</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = Q \Lambda Q^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0857709999999998em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord">Λ</span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>

<p>Here the decomposition is guaranteed to be real-valued and $Q$ is orthogonal.</p>

<p>The decomposition may not be unique if two (independent) eigenvectors have the same eigenvalues.</p>

<p>Zero-valued eigenvalues exist iff $A$ is singular.</p>

<h3 id="the-determinant">
<a class="anchor" href="#the-determinant" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Determinant</h3>

<p>The determinant, noted $det(A)$, is a scalar that reflects how much multiplying by $A$ alters the volume of an object.</p>

<p>The determinant is calculated by taking the product of $A$’s eigenvalues.</p>

<p>$det(A) = 1 \implies$ volume is preserved.</p>

<p>$det(A) = 0 \implies$ space is contracted completely in at least one dimension. Thus volume = 0.</p>

<h2 id="singular-value-decomposition">
<a class="anchor" href="#singular-value-decomposition" aria-hidden="true"><span class="octicon octicon-link"></span></a>Singular Value Decomposition</h2>

<h3 id="motivation-3">
<a class="anchor" href="#motivation-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>We want a decomposition for <em>all</em> real matrices. SVD provides this.</p>

<p>It is also key to computing the pseudoinverse (TODO), which enables us to find solutions to $Ax = b$ in all cases.</p>

<h3 id="singular-values--vectors">
<a class="anchor" href="#singular-values--vectors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Singular Values &amp; Vectors</h3>

<p>The SVD decomposes a matrix into:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>D</mi><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = UDV^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>

<p>where:</p>

<ul>
  <li>$U$ is an orthogonal $m \times m$ matrix of <strong>left-singular</strong> vectors (columns)</li>
  <li>$V$ is an orthogonal $n \times n$ matrix of <strong>right-singular</strong> vectors (columns)</li>
  <li>$D$ is a diagonal $m \times n$ matrix of <strong>singular values</strong> (not necessarily square)</li>
</ul>

<p>These matrices are calculated as follows:</p>

<ul>
  <li>$U$ = the (unit norm) eigenvectors of $A^TA$</li>
  <li>$V$ = the (unit norm) eigenvectors of $AA^T$</li>
  <li>$D$ = the square roots of the eigenvalues of either (padded with zeroes if there aren’t enough).</li>
</ul>

<p>With the SVD we can calculate the…</p>

<h2 id="pseudoinverse">
<a class="anchor" href="#pseudoinverse" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pseudoinverse</h2>

<h3 id="motivation-4">
<a class="anchor" href="#motivation-4" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>There are either 0, 1 or $\infty$ solutions to $Ax = b$ .</p>

<p>The inverse, $A^-1$, allows us to find $x$ if there is a singular solution.</p>

<p>However, for the inverse to exist, $A$ must satisfy the properties for being invertible (exactly $m$ linearly independent columns).</p>

<p>The pseudoinverse gives us a method for finding the “best” $x$ for all possible values of $A$ and $b$.</p>

<h3 id="the-moore-penrose-pseudoinverse">
<a class="anchor" href="#the-moore-penrose-pseudoinverse" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Moore-Penrose Pseudoinverse</h3>

<p>We denote the pseudoinverse as $A^+$ and take our candidate for $x$ as follows:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><msup><mi>A</mi><mo>+</mo></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">x^\prime = A^+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.801892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.821331em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mord mathdefault">b</span></span></span></span></span>

<p>The pseudoinverse is defined as follows:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mo>+</mo></msup><mo>=</mo><mi>V</mi><msup><mi>D</mi><mo>+</mo></msup><msup><mi>U</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A^+ = VD^+U^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.821331em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>

<p>where,</p>

<ul>
  <li>
    <p>$V$ and $U$ are taken directly from the SVD, although the order here is reversed</p>
  </li>
  <li>
    <p>$D^+$ contains the reciprocals of the (non-zero) values in $D$, with a transpose applied to fix the dimensions.</p>
  </li>
</ul>

<p>When there are $\infty$ valid solutions, the pseudoinverse gives the value of $x^\prime$ with minimal $L^2$ norm.</p>

<p>When there are 0 valid solutions, the pseudoinverse gives the value of $x^\prime$ such that $Ax^\prime - b$ has minimal $L^2$ norm.</p>

<h2 id="principal-component-analysis">
<a class="anchor" href="#principal-component-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Principal Component Analysis</h2>

<h3 id="motivation-5">
<a class="anchor" href="#motivation-5" aria-hidden="true"><span class="octicon octicon-link"></span></a>Motivation</h3>

<p>We often encounter high dimensional data that can be compressed by projecting it to a lower dimensional space.</p>

<p>We can do this using linear algebra, by multiplying by a matrix (encoding) and multiplying by its inverse (decoding).</p>

<p>We want to find the matrix that minimises information loss when encoding.</p>

<h3 id="process">
<a class="anchor" href="#process" aria-hidden="true"><span class="octicon octicon-link"></span></a>Process</h3>

<p>Suppose we have data $x^{(1)}, \dots , x^{(m)}$ where $x^{(i)} \in \mathbb{R}^n$.</p>

<p>Encoding/decoding works as follows:</p>

<ul>
  <li>Encoder: $f(\cdot)$</li>
  <li>Code: $c = f(x)$</li>
  <li>Decoder: $g(\cdot)$ where $x \approx g(c)$</li>
</ul>

<p>We first define the decoder: $g(c) = Dc$  where $D \in \mathbb{R^{n \times l}}$.</p>

<p>The columns of $D$ are orthogonal and have unit norm, but not square (so it’s not technically an orthogonal matrix).</p>

<p>The encoder is then defined: $f(x) = D^Tx$.</p>

<p>It can be shown this choice minimises $||x - g(c)||_{2}^{2}$.  (i.e. the gradient is zero when $c = D^Tx$)</p>

<p>The entire process defines the PCA reconstruction operation: $r(x) = DD^Tx$.</p>

<h3 id="choice-of-decoding-matrix">
<a class="anchor" href="#choice-of-decoding-matrix" aria-hidden="true"><span class="octicon octicon-link"></span></a>Choice of Decoding Matrix</h3>

<p>The only question that remains is what we should actually choose for the $l$ $n$-dimensional vectors that make up $D$.</p>

<p>It turns out the optimal choice here is the eigenvectors corresponding to the $l$ largest eigenvalues of $X^TX$ (i.e. squared singular values of $X$).</p>

<p>The proof of this relies on the following:</p>

<ol>
  <li>To find $d^{(1)}$ we seek to minimise $|| x^{(1)} - r(x^{(1)}) ||_F^2$.</li>
  <li>This can be shown to be equivalent to maximising $Tr\left(\frac{d^TX^TXd}{w^Tw}\right)$ subject to $d^Td = 1$.</li>
  <li>A standard result for a positive semidefinite matrix like $X^TX$ is that this fraction’s maximum possible value is equal to the largest eigenvalue of $X^TX$.</li>
  <li>To reach this possible value, $d^{(1)}$ is required to equal the corresponding eigenvector.</li>
  <li>This can be repeated for the subsequent values of $d$, which then correspond to the eigenvectors of the next smallest eigenvalues.</li>
</ol>

  </div><a class="u-url" href="/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Charlie Blake&#39;s personal blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
