---
layout: paper-notes
toc: true

title: ImageNet Classification with Deep Convolutional Neural Networks
description: Notes for the original AlexNet paper.
categories: [computer-vision, cnn]

paper-url: >-
  https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
bibtex: >-
  @inproceedings{krizhevsky2017imagenet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
    pages = {1097--1105},
    publisher = {Curran Associates, Inc.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    volume = {25},
    year = {2012}
  }
tag: "krizhevsky2017imagenet"
permalink: /paper-notes/krizhevsky2017imagenet.html
---

## Introduction

The paper focuses on the domain of visual object recognition. Interpreting high-dimensional pixel data is a difficult task, which requires learning complex relationships.

The authors outline two key components for addressing this:

1. Large datasets
2. Prior knowledge

These are addressed as follows:

1. Using the new ImageNet dataset, which has around $10^7$ images, versus $10^4/10^5$ for previous datasets.
2. Using the CNN architecture, whose inductive bias effectively encodes prior knowledge.

The paper makes the following contributions:

1. It demonstrates by far the best ImageNet results to date.
2. It presents a GPU-optimised implementation for CNNs that is key to scaling to larger datasets.

One key point which the authors perhaps don't stress enough is as follows:

> All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.

This observation appears key to the enduring success of such methods.

## The Dataset

ImageNet contains 15 million images of variable size, in roughly 22,000 categories. The images were taken from the web, and were labelled using Mechanical Turk.

The competition in question is the *ImageNet Large-Scale Visual Recognition Challenge* (ILSVRC), which has run since 2010. ILSVRC uses a subset of ImageNet with 1.2 million training images across 1000 categories.

On ImageNet the top-1 and top-5 error rates are typically reported, where 'top-n' denotes that the correct label is in the 'n' labels which a model gives the highest probability.

The only pre-processing is as follows:

1. Images are scaled to 256x256
2. Each pixel is normalised according to the 'mean activity' across the dataset[^1].

[^1]: It's not clear to me what is meant by 'mean activity' here. Does activity mean pixel value? And is this per-pixel or across all of them?

## Implementation

### Architecture 

The overall architecture contains 5 convolutional layers, followed by 3 fully-connected layers, and finally a softmax over the 1000 outputs.

The loss/objective is multinomial logistic regression.

### Key Features

The authors describe their key implementation features from most to least important:

#### ReLU Nonlinearity

In 2012 the standard nonlinearity was $tanh$, but based on {% cite nair2010rectified %} they use $ReLU$ instead.

They demonstrate a 6x speedup on CIFAR-10 (a smaller dataset) when comparing the two.

#### Training on Multiple GPUs

They spread the neurons across two GPUs. Each is able to read/write to the other's memory directly (i.e. without going through the main memory).

One trick they employ to make this work is to only have the GPUs communicate across certain layers. E.g. layer 2 is fully-connected to layer 3 across the two GPUs, but layer 3 is only fully-connected to layer 4 within the same GPU. This pattern is tuned as a hyperparameter (see the paper for an image detailing the final connectivity scheme, plus the layer/kernel dimensions).

This scheme reduces the error rates by (1.7%, 1.2%) compared with the single-GPU net.

#### Local Response Normalisation

One desirable property of ReLU is that is doesn't require input normalisation to prevent saturation. However, the authors still find an advantage to be gained by using local normalisation.

Their scheme localises each activation in the network by a factor based on the mean squared activations of adjacent activations. This is not done between all layers, but only the first two.

This scheme reduces the error rates by (1.4%, 1.2%).

#### Overlapping Pooling

Previous CNN architectures did not overlap pooling layers, but here they do. This scheme reduces the error rates by (0.4%, 0.3%).

## Reducing Overfitting

The model has 60 million parameters. The following techniques are designed to stop this large number of parameters from overfitting.

### Data Augmentation

Augmentation is effectively computationally 'free', as it done on the CPU while waiting for the previous batch to train on the GPU. The following techniques are used:

1. Translations and horizontal reflections.
2. Altering pixel intensities (using a fairly complex PCA-based scheme to change the intensity and colour of the overall image).

### Dropout

Ensemble learning is known to be a very successful way of improving model performance. The authors build on this by using the dropout technique introduced by {% cite hinton2012improving %}, which has a similar effect but using a single model.

For each batch, the output of each neuron is set to 0 with a probability of 0.5. The effectiveness of this is justified in the following way:

> This technique reduces complex co-adaptations of neurons,since a neuron cannot rely on the presence of particular other neurons.

Dropout is applied to the first two fully-connected layers.

### Weight Decay

The weight decay parameter is a small value: 0.0005. Nevertheless, this was found to be 'important for the model to learn'.

## Results

On ILSVRC-2010 the previous best result was (45.7%, 25.7%). The CNN model achieves **(37.5%, 17.0%)**.

On ILSVRC-2012 the second-best result was (. , 26.2%). The CNN model achieves **(. , 18.2%) **, and using an ensemble of 7 CNNs where 2 are pre-trained on ILSVRC-2011 achieves **(. , 15.3%)**.

## Discussion

The authors highlight the following points of note:

1. CNNs are a highly effective model architecture.
2. Their *depth* is key here, as removing even a single layer damages performance.
3. Only supervised learning is necessary to achieve these results.

They further speculate that:

1. Unsupervised pre-training could help significantly, especially in the case where the dataset stays the same size but the number of model parameters increases.
2. They are still several orders of magnitude away from the human visual system.
3. These networks could be especially powerful for video sequences, where the temporal structure provides further useful information.

## My Thoughts

### Why It Works

A few points seem notable in terms of why these methods have proved so effective. I will give these in order of significance:

1. These methods are designed to scale with compute and data: this presumably paved the way for CNNs to make such progress in computer vision.
2. The size of ImageNet: this dataset, which was relatively new at this point, is several orders of magnitude larger than previous datasets. This gave huge scope for a big breakthrough.
3. Hardware made this possible: training such a large model was only really possible because of GPUs. They again rode this wave to great effect.
4. Some neat tricks: ReLU, dropout, data augmentation, normalisation and weight decay have become central tools. Each of these knocked a small chunk off the error rate. Such tricks are very important and easily overlooked.
5. Big hyperparameter search: although exact details aren't reported, it sounds like the authors were able to throw a lot of compute at getting tuning many different aspects of the model, even down to the GPU communication. It should not be underestimated the extent to which throwing resources at this problem can yield substantial performance increases.

### The Paper Itself

The paper is well-structured, to-the-point and clear.

One criticism I have is with a lack of discussion of the methods they are comparing against though, which would have helped contextualise/justify their approach. I also would like more assurance that they are making a fair comparison with other methods. I'm not certain, for instance how the methods they compare against would fare if given an equal amount of compute (although given the success of CNNs here and subsequently, I would almost certainly expect them to do better under such conditions).

I would also have loved to see a proper table of ablations in an appendix (plus details of hyperparameter search). They do have some great ablation details in the body of the paper, but it would be good to see them all done thoroughly in one place.

### Thoughts for Further Reading

In a rough order of priority:

1. The original paper on dropout by {% cite hinton2012improving %}.
2. The original paper on ReLU by {% cite nair2010rectified %}.
3. Subsequent papers which improve on ILSVRC.
4. Other subsequent key papers in CV.
5. As suggested in the discussion, any work done using CNNs for video data.