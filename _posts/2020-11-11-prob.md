---
toc: true
layout: post
description: Notes for Chapter 1.3 in the book Deep Learning (Goodfellow et al.).
categories: [deep-learning-book, part-1, probability, information-theory]
title: "Deep Learning Book Notes: Probability & Information Theory"
---

Link to chapter: [1.3 Probability and Information Theory](https://www.deeplearningbook.org/contents/prob.html)

## Common Probability Distributions

### Bernoulli Distribution

**Support:**  {$0, 1$}

**Parameters:**  $0\leq p\leq 1$

**PMF:** 


$$
p(x) =
\begin{cases}
1-p &\quad x=0 \\
p   &\quad x=1
\end{cases}
$$

**CDF:**

$$
F(x) =
\begin{cases}
0   &\quad x \lt 0 \\
1-p &\quad 0\leq x \lt 1\\
p   &\quad x \ge 1
\end{cases}
$$


**Mean:**  $p$

**Variance:**  $p(1-p)$

### Multinoulli / Categorical Distribution

**Support:**  $x \in$ {$1, \dots, k$}

**Parameters:** 

- number of categories:  $k > 0$ 

- event probabilities: $p_1, \dots, p_k \quad (p_i \gt 0, \sum{p_i} = 1)$

**PFM:**


$$
p(x = i) = p_i
$$

### Normal / Gaussian Distribution

**Support:**  $x \in \mathbb{R}$

**Parameters:**  $\mu \in \mathbb{R}, \; \sigma^2 > 0$

**PDF:**


$$
p(x) = \frac {1}{\sigma {\sqrt {2\pi }}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}
$$


The CDF is more complex and cannot be expressed in terms of elementary functions.

The **central limit theorem** shows that the sum of many independent random variables is approximately normally distributed.

The following gives an interesting Bayesian interpretation of the normal distribution:

> Out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model.

Multivariate normal distribution, PDF:


$$
{\displaystyle (2\pi )^{-{\frac {k}{2}}}\det({\boldsymbol {\Sigma }})^{-{\frac {1}{2}}}\,e^{-{\frac {1}{2}}(\mathbf {x} -{\boldsymbol {\mu }})^{\!{\mathsf {T}}}{\boldsymbol {\Sigma }}^{-1}(\mathbf {x} -{\boldsymbol {\mu }})}}
$$


### Exponential Distribution

**Support:**  $x \in$ [$0, \infty$)

**Parameters:**  $\lambda > 0$

**PDF:**


$$
\lambda e^{-\lambda x}
$$


**CDF:**


$$
1 - e^{-\lambda x}
$$


One benefit of using this distribution is that it has a sharp peak at $x = 0$.

### Laplace Distribution

**Support:**  $x \in \mathbb{R}$

**Parameters:**  

- location:  $\mu \in \mathbb{R}$
- scale:  $b > 0$

**PDF:**


$$
p(x) = {\displaystyle {\frac {1}{2b}}\exp \left(-{\frac {|x-\mu |}{b}}\right)}
$$


**CDF:**


$$
F(x) = {\displaystyle {\begin{cases}{\frac {1}{2}}\exp \left({\frac {x-\mu }{b}}\right)&{\text{if }}x\leq \mu \\[8pt]1-{\frac {1}{2}}\exp \left(-{\frac {x-\mu }{b}}\right)&{\text{if }}x\geq \mu \end{cases}}}
$$


This is similar to the exponential distribution, but it allows us to place the peak anywhere we wish.

It is similar to the normal distribution too, but uses an absolute difference rather than the square.

### Dirac Distribution

If we wish to specify that all the mass in a probability distribution clusters around a single point then we can use the **Dirac delta function**, $\delta(x)$, which is zero-valued everywhere except 0, yet integrates to 1 (this is a special mathematical object called a *generalised function*).

**PDF:**  $p(x) = \delta(x - \mu)$

### Empirical Distribution

We can use the Dirac delta function with our training data, $x^{(1)}, \dots, x^{(m)}$, to define the following PDF:


$$
p(x) = \frac{1}{m}\sum^m_{i=1}{\delta(x - x^{(i)})}
$$


This concentrates all of the probability mass on the training data. In effect, this distribution represents the distribution that we sample from when we train a model on this dataset.

It is also the PDF that maximises the likelihood of the training data.

## Information Theory

### Information

The amount of information an event tells us, depends on its likelihood. Frequent events tell us little, while rare events tell us a lot.

Information theory gives us a measure called a **nat**, that quantifies how much information an event $x$ gives us. We denote this by $I(x)$.

Our requirements for such a function are that it satisfies the following:

- An event with probability 1 has $I(x) = 0$
- The less likely an event, the more information it transmits
- The information conveyed by independent events should be additive

We therefore define a nat as follows:


$$
I(x) = -\log{P(x)}
$$


Here we use the natural logarithm. If base 2, is used this measurement is called **shannons** or **bits**.

### Entropy

Moving to whole probability distributions, we define the expected information in an event sampled from a distribution as the **Shannon entropy**:


$$
H(x) = \mathbb{E}_{x \sim P}[I(x)]
$$


Distributions that are nearer deterministic have lower entropies, and distributions that are nearer uniform have higher entropies.

When $x$ is continuous this is also known as **differential entropy**.

### KL Divergence

If we want to compare the information in two probability distributions, we use the **Kullback-Leibler** divergence, which is the expected log probability ratio between the two distributions:


$$
D_{KL}(P || Q) = \mathbb{E}_{x \sim P}\left[\log\frac{P(x)}{Q(x)}\right]
$$


The KL divergence is $0$ when $P$ and $Q$ are the same.

This is sometimes thought of as a measure of "distance" between the two distributions. However, this measure is *not symmetric*, so does not satisfy the typical requirements of distances.

To visualise the asymmetry, see figure 6.3 in the book. The key point here is that if we wish to minimise the kl:

- **From the perspective of Q:** we want to make sure we have high probability whenever P has high probability (and if P has low probability, Q can be low or high)
- **From the perspective of P:** we want to make sure we have low probability whenever Q has low probability (and if Q has high probability, P can be low or high)

### Cross-Entropy

A similar measure is the cross-entropy, which is defined as:


$$
H(P, Q) = \mathbb{E}_{x \sim P}[-\log{Q(x)}]
$$


This measure can be thought of [in the following way](https://stats.stackexchange.com/a/265989):

> The cross entropy can be interpreted as the number of bits per message needed (on average) to encode events drawn from true distribution *p*, if using an optimal code for distribution *q*

Note that the cross entropy can be defined as the Shannon entropy of $P$ plus the KL divergence from $P$ to $Q$:


$$
H(P, Q) = H(P) + D_{KL}(P || Q)
$$

## Structured Probabilistic Models

### Motivation

The number of parameters in a probability distribution over $n$ random variables is exponential in $n$. Hence, using a single probability distributions over a large number of random variables can be very inefficient.

If we can factorise joint probability distributions into chains of conditional distributions, we can greatly reduce the number of parameters and computational cost.

We call these **structured probabilistic models** or **graphical models**.

### Directed Models

Given a graph $G$, we define the immediate parents of a node (as defined by the directed edges) as $Pa_G(x_i)$. We can then express the factorisation as follows:


$$
p(\mathbf{x}) = \prod_i{p(x_i|Pa_G(x_i))}
$$


The graph itself effectively encodes a number of (mainly conditional) independence relations between random variables. Specifically, any two nodes are conditionally independent given the values of their parents. This is really what we're exploiting to gain the efficiency speedup here.

### Undirected Models

In undirected models we associate *groups* of nodes with a factor.

We define a clique $C^{(i)}$ as a set of nodes that are all connected to one-another.

Each clique in the model is then associated with a factor $\phi^{(i)}(C^{(i)})$. Note that these factors are simply non-negative functions; *not* probability distributions.

To obtain the full joint probability distribution, we then multiply and normalise:
$$
p(\mathbf{x}) = \frac{1}{Z}\prod_i{\phi^{(i)}(C^{(i)})}
$$
where $Z$ is a normalising constant (i.e. the sum/integral of the probability over all outcomes). 

