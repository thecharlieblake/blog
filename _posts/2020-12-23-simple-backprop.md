---
keywords: fastai
description: Demonstrating the simplest possible backpropagation implementation, with all the clutter removed.
title: Bare-Bones Backpropagation
toc: true
badges: true
comments: true
author: Charlie Blake
categories: [neural-networks, backpropagation]
image: images/blog/simple-backprop/viz.png
nb_path: _notebooks/2020-12-23-simple-backprop.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-23-simple-backprop.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Making-Backprop-Simple">Making Backprop Simple<a class="anchor-link" href="#Making-Backprop-Simple"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first few times I came across backpropagation I struggled to get a feel for what was going on. It's not just enough
to follow the equations - I couldn't visualise the operations and updates in a nice, clean way.</p>
<p>If someone had shown me then how it's really just a generalisation of our old friend the chain rule--and barely any
more complex--then I think it would have helped me a lot. But instead I got bogged down in the matrix notation, and
figuring out what dimension went with what, and lost sight of what was really going on.</p>
<p>So this is a simple-as-possible backprop implementation. I don't go into the maths here. I assume the reader already knows what's going on in theory, but doesn't have a great feel for what happens in practice, when we have batches and layers and thesors and so on.</p>
<p>This can also serve as a reference for how to implement this from scratch in the nicest way. Enjoy!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First things first, there's some setup to do. This isn't a tutorial on data loading, so I'm just going to paste some
code for loading up our dataset and we can ignore the details. The only thing worth noting is that we'll be using the
classic <em>MNIST</em> dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">one_hot</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;data/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">transform</span><span class="o">=</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                             <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                             <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                         <span class="p">])</span>
                        <span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_sz</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This <code>train</code> dataloader can be iterated over and returns minibatches of shape <code>(batch__sz, input_dim)</code>.</p>
<p>In our case, these values are <code>(64, 28*28=784)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Layer-by-layer">Layer-by-layer<a class="anchor-link" href="#Layer-by-layer"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll construct our neural network by making classes for each layer. We'll use a standard setup of: linear layer, ReLU, linear layer, softmax; plus a cross-entropy loss.</p>
<p>For each layer class we require two methods:</p>
<ul>
<li><strong><code>__call__(self, x)</code></strong>: implements the forward pass. <code>__call__</code> allows us to feed an input through the layer by treating the initialised layer object as a function. For example: <code>relu_layer = ReLU(); output = relu_layer(input)</code>.</li>
</ul>
<ul>
<li><strong><code>bp_input(self, grad)</code></strong>: implements the backward pass, allowing us to backpropagate the gradient vector through the layer. The <code>grad</code> parameter is a matrix of partial derivatives of the loss, with respect to the data sent from the given layer to the next. As such, it is a <code>(batch_sz, out)</code> matrix. The job of <code>bp_input</code> is to return a <code>(batch_sz, in)</code> matrix to be sent to the next layer by multiplying <code>grad</code> by the derivative of the forward pass with respect to the <em>input</em> (or an equivalent operation).</li>
</ul>
<p>There are two other methods we sometimes wish to implement for different layers:</p>
<ul>
<li><strong><code>__init__(self, ...)</code></strong>: initialises the layer, e.g. weights.</li>
</ul>
<ul>
<li><strong><code>bp_param(self, grad)</code></strong>: the "final stop" of the backwards pass. Only applicable for layers with trainable weights. Similar to <code>bp_input</code>, but calculates the derivative with respect to the <em>weights</em> of the layer. Should return a matrix with the same shape as the weights (<code>self.W</code>) to be updated.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='The key point to recall when visualising this is that when we have a batch dimension it is always the first dimension. For both the forward and backward pass. This makes everything much simpler!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Linear-Layer">Linear Layer<a class="anchor-link" href="#Linear-Layer"> </a></h3><p>Let's start with the linear layer. We do the following:</p>
<ol>
<li>We start by initialising the weights (in this case using the Xavier initialisation).</li>
<li>We then implement the call method. Rather than adding an explicit bias, we append a vector of ones to the layer's input (this is equivalent, and makes backprop simpler).</li>
<li>Backpropagation with respect to the input is just right multiplication by the transpose of the weight matrix (adjusted to remove the added 1s column)</li>
<li>Backpropagation with respect to the output is left multiplication by the transpose of the input matrix.</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_sz</span><span class="p">,</span> <span class="n">out_sz</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_xavier_init</span><span class="p">(</span><span class="n">in_sz</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_sz</span><span class="p">)</span>  <span class="c1"># (in+1, out)</span>
    <span class="k">def</span> <span class="nf">_xavier_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span><span class="o">/</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">o</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>  <span class="c1"># (batch_sz, in)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_sz, in+1)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span>  <span class="c1"># (batch_sz, in+1) @ (in+1, out) = (batch_sz, out)</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">grad</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (batch_sz, out) @ (out, in) =  (batch_sz, in) </span>
    <span class="k">def</span> <span class="nf">bp_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">grad</span>  <span class="c1"># (in+1, batch_sz) @ (batch_sz, out) = (in+1, out)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="ReLU-Layer">ReLU Layer<a class="anchor-link" href="#ReLU-Layer"> </a></h3><p>Some non-linearity is a must! Bring on the RelU function. The implementation is pretty obvious here. <code>clamp()</code> is doing all the work.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (batch_sz, in)</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>  <span class="c1"># (batch_sz, in)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Softmax-&amp;-Cross-Entropy-Loss">Softmax &amp; Cross Entropy Loss<a class="anchor-link" href="#Softmax-&amp;-Cross-Entropy-Loss"> </a></h3><p>What? Both at once, why would you do this??</p>
<p>This is quite common, and I can justify it in two ways:</p>
<ol>
<li>This layer-loss combination often go together, so why not put them all in one layer? This saves us from having to do two separate forward and backward propagation steps.</li>
<li>I won't prove it here, but it turns out that the derivative of the loss with respect to the input to the softmax, is much simpler than the two intermediate derivative operations, and bypasses the numerical stability issues that arise when we do the exponential and the logarithm. Phew!</li>
</ol>
<p>The downside here is that is we're just doing <em>inference</em> then we only want the softmax output. But for the purposes of this tutorial we only really care about training. So this will do just fine!</p>
<p>There's a trick in the second line of the softmax implementation: it turns out subtracting the argmax from the softmax input keeps the output the same, but the intermediate values are more numerically stable. How neat!</p>
<p>Finally, we examine the backprop step. It's so simple! Our starting grad for backprop (the initial <code>grad</code> value passed in is just the ones vector) is the difference in our predicted output vector and the actual one-hot encoded label. This is so intuitive and wonderful.
{% include tip.html content='This is exactly the same derivative as when we don&#8217;t use a softmax layer and apply an MSE loss (i.e. the regression case). We can thus think of softmax + cross entropy as a way of getting to the same underlying backprop, but in the classification case.' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyLoss</span><span class="p">:</span>  <span class="c1"># (batch_sz, in=out) for all dims in this layer</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cross_entropy_loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="n">X_adj</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">exps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X_adj</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">exps</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Y_prob</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y_prob</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">bp_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Y_prob</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Y</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Putting-it-all-together">Putting it all together<a class="anchor-link" href="#Putting-it-all-together"> </a></h2><p>Let's bring these layers together in a class: our <code>NeuralNet</code> implementation.</p>
<p>The <code>evaluate()</code> function does two things. Firstly, it runs the forward pass by chaining the <code>__call__()</code> functions, to generate label probabilities. Secondly, it uses the labels passed to it to calculate the loss and percentage correctly predicted.
{% include note.html content='for this simplified example we don&#8217;t have a pure inference function, but we could add one with a small change to <code>SoftmaxCrossEntropyLoss</code>.' %}
The <code>gradient_descent()</code> function then gets the matrix of updates for each weight matrix and applies the update. The key bit here is how <code>backprop()</code> works. Going backwards through the computation graph we chain the backprop with respect to input methods. Then for each weighted layer we want to update, we apply the backprop with respect to prameters method to the relevant gradient vector.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">SoftmaxCrossEntropyLoss</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">Y_prob</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">one_hot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Y_prob</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">delta_W1</span><span class="p">,</span> <span class="n">delta_W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">delta_W1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">delta_W2</span>
    
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">d_z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_out</span><span class="p">)</span>
        <span class="n">d_a1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_z2</span><span class="p">)</span>
        <span class="n">d_z1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="o">.</span><span class="n">bp_input</span><span class="p">(</span><span class="n">d_a1</span><span class="p">)</span>

        <span class="n">d_w2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="o">.</span><span class="n">bp_param</span><span class="p">(</span><span class="n">d_z2</span><span class="p">)</span>
        <span class="n">d_w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="o">.</span><span class="n">bp_param</span><span class="p">(</span><span class="n">d_z1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">d_w1</span><span class="p">,</span> <span class="n">d_w2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-model">Training the model<a class="anchor-link" href="#Training-the-model"> </a></h2><p>We're almost there! I won't go into this bit too much because this tutorial isn't about training loops, but it's all very standard here.</p>
<p>We break the training data into minibatches and train on them over 10 epochs. The evaluation metrics plotted are those recorded during regular training.
{% include warning.html content='these results are only on the training set! In practice we should <em>always</em> plot performance on a test set, but we don&#8217;t want to clutter the tutorial with this extra detail.' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">()</span>
<span class="n">stats</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;correct&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train</span><span class="p">):</span>
        <span class="n">y_prob</span><span class="p">,</span> <span class="n">batch_correct</span><span class="p">,</span> <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">gradient_descent</span><span class="p">()</span>
        
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">batch_correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">batch_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">stats</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1"> | correct: </span><span class="si">{</span><span class="n">correct</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">stats</span><span class="p">))</span><span class="o">.</span><span class="n">mark_line</span><span class="p">()</span> \
          <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)))</span>
<span class="n">line1</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">stroke</span><span class="o">=</span><span class="s1">&#39;#5276A7&#39;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;monotone&#39;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span>   <span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Loss&#39;</span>   <span class="p">,</span> <span class="n">titleColor</span><span class="o">=</span><span class="s1">&#39;#5276A7&#39;</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span>   <span class="p">])])),</span> <span class="n">tooltip</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span>   <span class="p">)</span>
<span class="n">line2</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">stroke</span><span class="o">=</span><span class="s1">&#39;#57A44C&#39;</span><span class="p">,</span> <span class="n">interpolate</span><span class="o">=</span><span class="s1">&#39;monotone&#39;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s1">&#39;correct&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Axis</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Correct&#39;</span><span class="p">,</span> <span class="n">titleColor</span><span class="o">=</span><span class="s1">&#39;#57A44C&#39;</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Scale</span><span class="p">(</span><span class="n">domain</span><span class="o">=</span><span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]),</span> <span class="mf">1.0</span><span class="p">])),</span> <span class="n">tooltip</span><span class="o">=</span><span class="s1">&#39;correct&#39;</span><span class="p">)</span>
<span class="n">alt</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">line1</span><span class="p">,</span> <span class="n">line2</span><span class="p">)</span><span class="o">.</span><span class="n">resolve_scale</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;independent&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch: 0 | correct: 0.87, loss: 0.47
epoch: 1 | correct: 0.92, loss: 0.28
epoch: 2 | correct: 0.93, loss: 0.23
epoch: 3 | correct: 0.94, loss: 0.20
epoch: 4 | correct: 0.95, loss: 0.18
epoch: 5 | correct: 0.95, loss: 0.16
epoch: 6 | correct: 0.96, loss: 0.15
epoch: 7 | correct: 0.96, loss: 0.14
epoch: 8 | correct: 0.96, loss: 0.13
epoch: 9 | correct: 0.96, loss: 0.12
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-baa92da218ff4d3cb41d464cac84b660"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== "altair-viz-baa92da218ff4d3cb41d464cac84b660") {
      outputDiv = document.getElementById("altair-viz-baa92da218ff4d3cb41d464cac84b660");
    }
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "layer": [{"mark": {"type": "line", "interpolate": "monotone", "stroke": "#5276A7"}, "encoding": {"tooltip": {"type": "quantitative", "field": "loss"}, "x": {"type": "quantitative", "axis": {"title": "epoch"}, "field": "epoch"}, "y": {"type": "quantitative", "axis": {"title": "Loss", "titleColor": "#5276A7"}, "field": "loss", "scale": {"domain": [0.0, 0.47194650769233704]}}}}, {"mark": {"type": "line", "interpolate": "monotone", "stroke": "#57A44C"}, "encoding": {"tooltip": {"type": "quantitative", "field": "correct"}, "x": {"type": "quantitative", "axis": {"title": "epoch"}, "field": "epoch"}, "y": {"type": "quantitative", "axis": {"title": "Correct", "titleColor": "#57A44C"}, "field": "correct", "scale": {"domain": [0.8700526385927538, 1.0]}}}}], "data": {"name": "data-551adda8aec31a1fe519546b549c9e76"}, "resolve": {"scale": {"y": "independent"}}, "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json", "datasets": {"data-551adda8aec31a1fe519546b549c9e76": [{"correct": 0.8700526385927538, "loss": 0.47194650769233704, "epoch": 0}, {"correct": 0.9205423773987237, "loss": 0.2753719389438629, "epoch": 1}, {"correct": 0.9332855810234594, "loss": 0.22961291670799255, "epoch": 2}, {"correct": 0.9432469349680197, "loss": 0.19969312846660614, "epoch": 3}, {"correct": 0.9497101545842259, "loss": 0.17908376455307007, "epoch": 4}, {"correct": 0.953991204690836, "loss": 0.16318272054195404, "epoch": 5}, {"correct": 0.9573727345415821, "loss": 0.15032540261745453, "epoch": 6}, {"correct": 0.960104610874205, "loss": 0.13999834656715393, "epoch": 7}, {"correct": 0.9624700159914752, "loss": 0.1315470188856125, "epoch": 8}, {"correct": 0.9643356876332664, "loss": 0.12460409104824066, "epoch": 9}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And our results are great! After 10 epochs I'm getting a whopping 97% correct.</p>
<p>Given we've implemented this all from scratch, backprop included, to get these results using only 4 notebook cells worth of code is a testament to how simple backprop really is!</p>

</div>
</div>
</div>
</div>
 

