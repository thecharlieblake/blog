<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ImageNet Classification with Deep Convolutional Neural Networks | Charlie Blake: Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="ImageNet Classification with Deep Convolutional Neural Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes for the original AlexNet paper." />
<meta property="og:description" content="Notes for the original AlexNet paper." />
<link rel="canonical" href="https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html" />
<meta property="og:url" content="https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html" />
<meta property="og:site_name" content="Charlie Blake: Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-02T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html"},"headline":"ImageNet Classification with Deep Convolutional Neural Networks","dateModified":"2020-11-02T00:00:00-06:00","datePublished":"2020-11-02T00:00:00-06:00","description":"Notes for the original AlexNet paper.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://thecharlieblake.co.uk/feed.xml" title="Charlie Blake: Blog" /><link rel="shortcut icon" type="image/png" href="/images/favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Charlie Blake: Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Paper Notes: ImageNet Classification with Deep Convolutional Neural Networks</h1><p class="page-description">Notes for the original AlexNet paper.</p><style type="text/css">
      div.bibtex code {
        font-size: 12px !important
      }
      div.bibtex figure.highlight {
        margin: 1em auto
      }
    </style>

    <div>
      <span id="krizhevsky2017imagenet">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q. Weinberger (Eds.), <i>Advances in Neural Information Processing Systems</i> (Vol. 25, pp. 1097–1105). Curran Associates, Inc.</span>
    </div>

    <div>
      <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>
    </div>

    <div class="bibtex">
      
<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="o">@</span><span class="n">inproceedings</span><span class="p">{</span><span class="n">krizhevsky2017imagenet</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Krizhevsky</span><span class="p">,</span> <span class="n">Alex</span> <span class="n">and</span> <span class="n">Sutskever</span><span class="p">,</span> <span class="n">Ilya</span> <span class="n">and</span> <span class="n">Hinton</span><span class="p">,</span> <span class="n">Geoffrey</span> <span class="n">E</span><span class="p">},</span>
  <span class="n">booktitle</span> <span class="o">=</span> <span class="p">{</span><span class="n">Advances</span> <span class="n">in</span> <span class="n">Neural</span> <span class="n">Information</span> <span class="n">Processing</span> <span class="n">Systems</span><span class="p">},</span>
  <span class="n">editor</span> <span class="o">=</span> <span class="p">{</span><span class="n">F</span><span class="py">. Pereira</span> <span class="n">and</span> <span class="n">C</span><span class="py">. J. C. Burges</span> <span class="n">and</span> <span class="n">L</span><span class="py">. Bottou</span> <span class="n">and</span> <span class="n">K</span><span class="py">. Q. Weinberger</span><span class="p">},</span>
  <span class="n">pages</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1097</span><span class="o">--</span><span class="mi">1105</span><span class="p">},</span>
  <span class="n">publisher</span> <span class="o">=</span> <span class="p">{</span><span class="n">Curran</span> <span class="n">Associates</span><span class="p">,</span> <span class="n">Inc</span><span class="err">.</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">ImageNet</span> <span class="n">Classification</span> <span class="n">with</span> <span class="n">Deep</span> <span class="n">Convolutional</span> <span class="n">Neural</span> <span class="n">Networks</span><span class="p">},</span>
  <span class="n">volume</span> <span class="o">=</span> <span class="p">{</span><span class="mi">25</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2012</span><span class="p">}</span>
<span class="p">}</span></code></pre></figure>

    </div>
    
    <div class="cell border-box-sizing code_cell rendered" style="margin: -0.5rem 0 0.5rem 0 !important">
      <details class="description">
        <summary class="btn btn-sm" data-open="Hide PDF" data-close="Show PDF"></summary>
        <p></p>
        <div class="input">
          <div class="inner_cell">
            <div class="input_area">
              <iframe src="https://docs.google.com/gview?url=https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&embedded=true" style="width:100%; height:100vh;" frameborder="0"></iframe>
            </div>
          </div>
        </div>
      </details>
    </div>

    <p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-02T00:00:00-06:00" itemprop="datePublished">
        Nov 2, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#computer-vision">computer-vision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#cnn">cnn</a>
        
      
      </p>
    

    </header>


  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#the-dataset">The Dataset</a></li>
<li class="toc-entry toc-h2"><a href="#implementation">Implementation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#architecture">Architecture</a></li>
<li class="toc-entry toc-h3"><a href="#key-features">Key Features</a>
<ul>
<li class="toc-entry toc-h4"><a href="#relu-nonlinearity">ReLU Nonlinearity</a></li>
<li class="toc-entry toc-h4"><a href="#training-on-multiple-gpus">Training on Multiple GPUs</a></li>
<li class="toc-entry toc-h4"><a href="#local-response-normalisation">Local Response Normalisation</a></li>
<li class="toc-entry toc-h4"><a href="#overlapping-pooling">Overlapping Pooling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#reducing-overfitting">Reducing Overfitting</a>
<ul>
<li class="toc-entry toc-h3"><a href="#data-augmentation">Data Augmentation</a></li>
<li class="toc-entry toc-h3"><a href="#dropout">Dropout</a></li>
<li class="toc-entry toc-h3"><a href="#weight-decay">Weight Decay</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#results">Results</a></li>
<li class="toc-entry toc-h2"><a href="#discussion">Discussion</a></li>
<li class="toc-entry toc-h2"><a href="#my-thoughts">My Thoughts</a>
<ul>
<li class="toc-entry toc-h3"><a href="#why-it-works">Why It Works</a></li>
<li class="toc-entry toc-h3"><a href="#the-paper-itself">The Paper Itself</a></li>
<li class="toc-entry toc-h3"><a href="#thoughts-for-further-reading">Thoughts for Further Reading</a></li>
</ul>
</li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>The paper focuses on the domain of object recognition.</p>

<p>This is a very complex task. The authors outline two key components for dealing with this:</p>

<ol>
  <li>Large datasets</li>
  <li>Prior knowledge</li>
</ol>

<p>These are addressed as follows:</p>

<ol>
  <li>Using the new ImageNet dataset, which has around $10^7$ images, versus $10^4/10^5$ for previous datasets.</li>
  <li>Using the CNN architecture, whose inductive bias effectively encodes prior knowledge.</li>
</ol>

<p>The paper makes the following contributions:</p>

<ol>
  <li>It demonstrates by far the best ImageNet results to date.</li>
  <li>It presents a GPU-optimised implementation for CNNs that is key to scaling to larger datasets.</li>
</ol>

<p>One key point which the authors perhaps don’t stress enough is as follows:</p>

<blockquote>
  <p>l of our experiments suggest that our resultscan be improved simply by waiting for faster GPUs and bigger datasets to become available.</p>
</blockquote>

<p>This observation would seem to be key to the enduring success of such methods.</p>

<h2 id="the-dataset">
<a class="anchor" href="#the-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Dataset</h2>

<p>ImageNet contains 15 million images of variable size, in roughly 22,000 categories. The images were taken from the web, and were labelled using Mechanical Turk.</p>

<p>The competition in question is the <em>ImageNet Large-Scale Visual Recognition Challenge</em> (ILSVRC), which has run since 2010. ILSVRC uses a subset of ImageNet with 1.2 million training images across 1000 categories.</p>

<p>On ImageNet the top-1 and top-5 error rates are typically reported, where ‘top-n’ denotes that the correct label is in the ‘n’ labels which a model gives the highest probability.</p>

<p>The only pre-processing is as follows:</p>

<ol>
  <li>Images are scaled to 256x256</li>
  <li>Each pixel is normalised according to the ‘mean activity’ across the dataset<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>.</li>
</ol>

<h2 id="implementation">
<a class="anchor" href="#implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation</h2>

<h3 id="architecture">
<a class="anchor" href="#architecture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Architecture</h3>

<p>The overall architecture contains 5 convolutional layers, followed by 3 fully-connected layers, and finally a softmax over the 1000 outputs.</p>

<p>The loss/objective is multinomial logistic regression.</p>

<h3 id="key-features">
<a class="anchor" href="#key-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key Features</h3>

<p>The authors describe their key implementation features from most to least important:</p>

<h4 id="relu-nonlinearity">
<a class="anchor" href="#relu-nonlinearity" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLU Nonlinearity</h4>

<p>In 2012 the standard nonlinearity was $tanh$, but based on <a class="citation" href="#nair2010rectified">(Nair &amp; Hinton, 2010)</a> they use $ReLU$ instead.</p>

<p>They demonstrate a 6x speedup on CIFAR-10 (a smaller dataset) when comparing the two.</p>

<h4 id="training-on-multiple-gpus">
<a class="anchor" href="#training-on-multiple-gpus" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training on Multiple GPUs</h4>

<p>They spread the neurons across two GPUs. Each is able to read/write to the other’s memory directly (i.e. without going through the main memory).</p>

<p>One trick they employ to make this work is to only have the GPUs communicate across certain layers. E.g. layer 2 is fully-connected to layer 3 across the two GPUs, but layer 3 is only fully-connected to layer 4 within the same GPU. This pattern is tuned as a hyperparameter (see the paper for an image detailing the final connectivity scheme, plus the layer/kernel dimensions).</p>

<p>This scheme reduces the error rates by (1.7%, 1.2%) compared with the single-GPU net.</p>

<h4 id="local-response-normalisation">
<a class="anchor" href="#local-response-normalisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local Response Normalisation</h4>

<p>One desirable property of ReLU is that is doesn’t require input normalisation to prevent saturation. However, the authors still find an advantage to be gained by using local normalisation.</p>

<p>Their scheme localises each activation in the network by a factor based on the mean squared activations of adjacent activations. This is not done between all layers, but only the first two.</p>

<p>This scheme reduces the error rates by (1.4%, 1.2%).</p>

<h4 id="overlapping-pooling">
<a class="anchor" href="#overlapping-pooling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overlapping Pooling</h4>

<p>Previous CNN architectures did not overlap pooling layers, but here they do. This scheme reduces the error rates by (0.4%, 0.3%).</p>

<h2 id="reducing-overfitting">
<a class="anchor" href="#reducing-overfitting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reducing Overfitting</h2>

<p>The model has 60 million parameters. The following techniques are designed to stop this large number of parameters from overfitting.</p>

<h3 id="data-augmentation">
<a class="anchor" href="#data-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Augmentation</h3>

<p>Augmentation is effectively computationally ‘free’, as it done on the CPU while waiting for the previous batch to train on the GPU. The following techniques are used:</p>

<ol>
  <li>Translations and horizontal reflections.</li>
  <li>Altering pixel intensities (using a fairly complex PCA-based scheme to change the intensity and colour of the overall image).</li>
</ol>

<h3 id="dropout">
<a class="anchor" href="#dropout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dropout</h3>

<p>Ensemble learning is known to be a very successful way of improving model performance. The authors build on this by using the dropout technique introduced by <a class="citation" href="#hinton2012improving">(Hinton et al., 2012)</a>, which has a similar effect but using a single model.</p>

<p>For each batch, the output of each neuron is set to 0 with a probability of 0.5. The effectiveness of this is justified in the following way:</p>

<blockquote>
  <p>This technique reduces complex co-adaptations of neurons,since a neuron cannot rely on the presence of particular other neurons.</p>
</blockquote>

<p>Dropout is applied to the first two fully-connected layers.</p>

<h3 id="weight-decay">
<a class="anchor" href="#weight-decay" aria-hidden="true"><span class="octicon octicon-link"></span></a>Weight Decay</h3>

<p>The weight decay parameter is a small value: 0.0005. Nevertheless, this was found to be ‘important for the model to learn’.</p>

<h2 id="results">
<a class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<p>On ILSVRC-2010 the previous best result was (45.7%, 25.7%). The CNN model achieves <strong>(37.5%, 17.0%)</strong>.</p>

<p>On ILSVRC-2012 the second-best result was (_, 26.2%). The CNN model achieves <strong>(__, 18.2%) **, and using an ensemble of 7 CNNs where 2 are pre-trained on ILSVRC-2011 achieves **(__, 15.3%)</strong>.</p>

<h2 id="discussion">
<a class="anchor" href="#discussion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion</h2>

<p>The authors highlight the following points of note:</p>

<ol>
  <li>CNNs are a highly effective model architecture.</li>
  <li>Their <em>depth</em> is key here, as removing even a single layer damages performance.</li>
  <li>Only supervised learning is necessary to achieve these results.</li>
</ol>

<p>They further speculate that:</p>

<ol>
  <li>Unsupervised pre-training could help significantly, especially in the case where the dataset stays the same size but the number of model parameters increases.</li>
  <li>They are still several orders of magnitude away from the human visual system.</li>
  <li>These networks could be especially powerful for video sequences, where the temporal structure provides further useful information.</li>
</ol>

<h2 id="my-thoughts">
<a class="anchor" href="#my-thoughts" aria-hidden="true"><span class="octicon octicon-link"></span></a>My Thoughts</h2>

<h3 id="why-it-works">
<a class="anchor" href="#why-it-works" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why It Works</h3>

<p>A few points seem notable in terms of why these methods have proved so effective. I will give these in order of significance:</p>

<ol>
  <li>These methods are designed to scale with compute and data: this presumably paved the way for CNNs to make such progress in computer vision.</li>
  <li>The size of ImageNet: this dataset, which was relatively new at this point, is several orders of magnitude larger than previous datasets. This gave huge scope for a big breakthrough.</li>
  <li>Hardware made this possible: training such a large model was only really possible because of GPUs. They again rode this wave to great effect.</li>
  <li>Some neat tricks: ReLU, dropout, data augmentation, normalisation and weight decay have become central tools. Each of these knocked a small chunk off the error rate. Such tricks are very important and easily overlooked.</li>
  <li>Big hyperparameter search: although exact details aren’t reported, it sounds like the authors were able to throw a lot of compute at getting tuning many different aspects of the model, even down to the GPU communication. It should not be underestimated the extent to which throwing resources at this problem can yield substantial performance increases.</li>
</ol>

<h3 id="the-paper-itself">
<a class="anchor" href="#the-paper-itself" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Paper Itself</h3>

<p>The paper is well-structured, to-the-point and clear. There is nothing overly complex here.</p>

<p>One criticism is with a lack of discussion of the methods they are comparing against though, which would have helped contextualise/justify their approach. I also would like more assurance that they are making a fair comparison with other methods. I’m not certain, for instance how the methods they compare against would fare if given an equal amount of compute (although given the success of CNNs here and subsequently, I would almost certainly expect them to do better under such conditions).</p>

<p>I would also have loved to see a proper table of ablations in an appendix (plus details of hyperparameter search). They do have some great ablation details in the body of the paper, but it would be good to see them all done thoroughly in one place.</p>

<h3 id="thoughts-for-further-reading">
<a class="anchor" href="#thoughts-for-further-reading" aria-hidden="true"><span class="octicon octicon-link"></span></a>Thoughts for Further Reading</h3>

<p>In a rough order of priority:</p>

<ol>
  <li>The original paper on dropout by <a class="citation" href="#hinton2012improving">(Hinton et al., 2012)</a>.</li>
  <li>The original paper on ReLU by <a class="citation" href="#nair2010rectified">(Nair &amp; Hinton, 2010)</a>.</li>
  <li>Subsequent papers which improve on ILSVRC.</li>
  <li>Other subsequent key papers in CV.</li>
  <li>As suggested in the discussion, any work done using CNNs for video data.</li>
</ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It’s not clear to me what is meant by ‘mean activity’ here. Does activity mean pixel value? And is this per-pixel or across all of them? <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

    <ol class="bibliography"><li><span id="nair2010rectified">Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. <i>ICML</i>.</span></li>
<li><span id="hinton2012improving">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. <i>ArXiv Preprint ArXiv:1207.0580</i>.</span></li></ol>
  </div><a class="u-url" href="/paper-notes/krizhevsky2017imagenet.html" hidden></a>
</article>



      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Charlie Blake&#39;s personal blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/thecharlieblake" title="thecharlieblake"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
