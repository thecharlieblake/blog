{
  
    
        "post0": {
            "title": "Deep Learning Book Notes: Linear Algebra",
            "content": "Link to chapter: 1.2 Linear Algebra . Matrix Multiplication . Key Terms . Span (of a matrix / set of vectors): the set of all vectors obtainable via linear combination . Column / row space: span of a matrix’s columns / rows . Linear independence: no vectors are in the span of any other group of vectors . Invertible matrix: square matrix with linearly independent columns . Motivation . Consider the “basic” matrix-vector multiplication equation: $Ax = b$, where $A$ is a fixed matrix and $x$ is a variable vector to be found. . For a fixed $b$, we care about the possible solutions (i.e. values of $x$): how many are there and what are they? . Considering the case where $b$ is arbitrary (i.e. considering what is true for all $b$) is perhaps more interesting, and can tell us a lot about $A$ and its properties. The key question is: does the equation have a solution for all $b$? . Solutions . In the case of a fixed $b$ the basic equation either has 0, 1 or $ infty$ many solutions. The authors provide a useful way of thinking about this: . To analyze how many solutions the equation has, think of the columns of $A$ as specifying diﬀerent directions we can travel in from the origin … then determine how many ways there are of reaching $b$. . In the case of an arbitrary $b$: . There is &gt;= 1 solution for all $b$ iff $A$ has a set of $m$ linearly independent columns. This is due to the column space of $A$ being all of $ mathbb{R}^{m}$. | A necessary (but not sufficient) condition here is $n gt m$ (at least as many columns as rows), otherwise the column space can’t span $ mathbb{R}^{m}$. | . | There is = 1 solution for all $b$ iff $A$ has exactly $m$ linearly independent columns. A necessary condition is therefore that m = n (i.e. $A$ is square). | If $A$ satisfies this condition we say it is invertible. | Note that a square matrix that is not invertible is called singular or degenerate. | . | . Why is this all useful? Consider… . Inverse Matrices . Think of $A$ as applying a transformation to a vector or matrix. . If the basic equation has one solution (i.e. $A$ is invertible) then this transformation can be reversed. This is often really useful! . This reversal can be expressed as the matrix inverse $A^{-1}$. . In practice computing $A^{-1}$ directly is often avoided as it can be numerically unstable, but this property is still very important. . Norms . Motivation . A norm is a function that gives us some measure of the distance from the origin to a vector or matrix. . Clearly this is a useful concept! . Definition . A norm is any function which satisfies the following three properties (for all $ alpha, x, y$): . point-separating: $f(x) = 0 implies x = 0$ . absolutely scalable: $f( alpha x) = | alpha|f(x)$ . triangle inequality: $f(x + y) &lt;= f(x) + f(y)$ . Vector Norms . The $L^p$ norm of $x$, often denoted by $||x||_p$ , is defined as: . ∣∣x∣∣p=(∑i∣xi∣p)1p||x||_p = left( sum_i |x_i|^p right)^{ frac{1}{p}} quad∣∣x∣∣p​=(i∑​∣xi​∣p)p1​ . where ($p in mathbb{R}, p ge 1$) . . The $L^1$ norm is called the Manhattan norm. . The $L^2$ norm is called the Euclidean norm. This is the standard norm and is commonly referred to without the subscript as simply $||x||$. The squared $L^2$ norm is also used in some contexts, which is simply $x^Tx$. . The $L^ infty$ norm is called the max norm. It is defined as $||x||_ infty = max_i{|x_i|}$. . Matrix Norms . (This is a much more complex field that we only touch on briefly here!) . We consider two analogous matrix norms for the $L^2$ vector norm. . In wider mathematics the spectral norm is often used. It can be useful in ML for analysing (among other things) exploding/vanishing gradients. It is defined as $A$’s largest singular value: $||A|| = sigma_{ max}{(A)}$ . However, most in most ML applications it is assumed the Frobenius norm is used. This is defined as: . ∥∥A∥∥F=∑i,jAi,j2 | |A | |_F = sqrt{ sum_{i,j}{A^2_{i,j}}}∥∥A∥∥F​=i,j∑​Ai,j2​ . ​ . Note that this is equivalent to: $||A||_F = sqrt{Tr(AA^T)}$ . Eigendecomposition . Key Terms . Unit vector: the $L^2$ norm = 1 . Orthogonal vectors: $x^Ty = 0$ . Orthonormal vectors: orthogonal unit vectors . Orthogonal matrix: rows &amp; columns are mutually orthonormal . Motivation . Decomposing matrices can help us learn about a matrix by breaking it down into its constituent parts. This can reveal useful properties about the matrix. . Eigendecomposition decomposes a matrix into eigenvectors and eigenvalues. . These tell us something about the directions and sizes of the transformation created when multiplying by the matrix. . Eigenvectors &amp; Eigenvalues . Vector $v$ and scalar $ lambda$ are a eigenvector-eigenvalue pair for square matrix $A$ iff: . $v neq mathbf{0}$ | $Av = lambda v$ | (Strictly speaking, here $v$ is a right eigenvector. A left eigenvector is such that $v^TA = lambda v^T$. We care primarily about right eigenvectors.) . If $v$ is an eigenvector it follows that any rescaled version of $v$ is also an eigenvector with the same eigenvalue. We typically use a scale such that we have a unit eigenvector. . If $A$ has $n$ independent eigenvectors we can create a matrix of them, $V$, such that $AV = V diag( lambda)$. . The eigendecomposition of $A$ is then defined as: $A = V diag( lambda) V^{-1}$. . We are only guaranteed an eigendecomposition if $A$ is symmetric (and real-valued). In this case it is often denoted: . A=QΛQTA = Q Lambda Q^TA=QΛQT . Here the decomposition is guaranteed to be real-valued and $Q$ is orthogonal. . The decomposition may not be unique if two (independent) eigenvectors have the same eigenvalues. . Zero-valued eigenvalues exist iff $A$ is singular. . The Determinant . The determinant, noted $det(A)$, is a scalar that reflects how much multiplying by $A$ alters the volume of an object. . The determinant is calculated by taking the product of $A$’s eigenvalues. . $det(A) = 1 implies$ volume is preserved. . $det(A) = 0 implies$ space is contracted completely in at least one dimension. Thus volume = 0. . Singular Value Decomposition . Motivation . We want a decomposition for all real matrices. SVD provides this. . It is also key to computing the pseudoinverse (TODO), which enables us to find solutions to $Ax = b$ in all cases. . Singular Values &amp; Vectors . The SVD decomposes a matrix into: . A=UDVTA = UDV^TA=UDVT . where: . $U$ is an orthogonal $m times m$ matrix of left-singular vectors | $V$ is an orthogonal $n times n$ matrix of right-singular vectors | $D$ is a diagonal $m times n$ matrix of singular values (not necessarily square) | . These matrices are calculated as follows: . $U$ = the (unit norm) eigenvectors of $A^TA$ | $V$ = the (unit norm) eigenvectors of $AA^T$ | $D$ = the square roots of the eigenvalues of either (padded with zeroes if there aren’t enough). | . With the SVD we can calculate the… . Pseudoinverse . Motivation . There are either 0, 1 or $ infty$ solutions to $Ax = b$ . . The inverse, $A^-1$, allows us to find $x$ if there is a singular solution. . However, for the inverse to exist, $A$ must satisfy the properties for being invertible (exactly $m$ linearly independent columns). . The pseudoinverse gives us a method for finding the “best” $x$ for all possible values of $A$ and $b$. . The Moore-Penrose Pseudoinverse . We denote the pseudoinverse as $A^+$ and take our candidate for $x$ as follows: . x′=A+bx^ prime = A^+bx′=A+b . The pseudoinverse is defined as follows: . A+=VD+UTA^+ = VD^+U^TA+=VD+UT . where, . $V$ and $U$ are taken directly from the SVD, although the order here is reversed . | $D^+$ contains the reciprocals of the (non-zero) values in $D$, with a transpose applied to fix the dimensions. . | . When there are $ infty$ valid solutions, the pseudoinverse gives the value of $x^ prime$ with minimal $L^2$ norm. . When there are 0 valid solutions, the pseudoinverse gives the value of $x^ prime$ such that $Ax^ prime - b$ has minimal $L^2$ norm. . Principal Component Analysis . Motivation . We often encounter high dimensional data that can be compressed by projecting it to a lower dimensional space. . We can do this using linear algebra, by multiplying by a matrix (encoding) and multiplying by its inverse (decoding). . We want to find the matrix that minimises information loss when encoding. . Process . Suppose we have data $x^{(1)}, dots , x^{(m)}$ where $x^{(i)} in mathbb{R}^n$. . Encoding/decoding works as follows: . Encoder: $f( cdot)$ | Code: $c = f(x)$ | Decoder: $g( cdot)$ where $x approx g(c)$ | . We first define the decoder: $g(c) = Dc$ where $D in mathbb{R^{n times l}}$. . The columns of $D$ are orthogonal and have unit norm, but not square (so it’s not technically an orthogonal matrix). . The encoder is then defined: $f(x) = D^Tx$. . It can be shown this choice minimises $ |   | x - g(c) |   | _{2}^{2}$. (i.e. the gradient is zero when $c = D^Tx$) | . The entire process defines the PCA reconstruction operation: $r(x) = DD^Tx$. . Choice of Decoding Matrix . The only question that remains is what we should actually choose for the $l$ $n$-dimensional vectors that make up $D$. . It turns out the optimal choice here is the eigenvectors corresponsing to the $l$ largest eigenvalues of $X^TX$ (i.e. squared singular values of $X$). . The proof of this relies on the following: . To find $d^{(1)}$ we seek to minimise $|| x^{(1)} - r(x^{(1)}) ||_F^2$. | This can be shown to be equivalent to maximising $Tr left( frac{d^TX^TXd}{w^Tw} right)$ subject to $d^Td = 1$. | A standard result for a positive semidefinite matrix like $X^TX$ is that this fraction’s maximum possible value is equal to the largest eigenvalue of $X^TX$. | To reach this possible value, $d^{(1)}$ is required to equal the corresponding eigenvector. | This can be repeated for the subsequent values of $d$, which then correspond to the eigenvectors of the next smallest eigenvalues. |",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "relUrl": "/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "content": "Introduction . Convnets already introduced | Massively outperforms previous models | etc | . The Dataset . Imagenet much larger than previous datasets . | … . ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000categories. The images were collected from the web and labeled by human labelers using Ama-zon’s Mechanical Turk crowd-sourcing tool. . | .",
            "url": "https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html",
            "relUrl": "/paper-notes/krizhevsky2017imagenet.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thecharlieblake.co.uk/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thecharlieblake.co.uk/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "As of October 2020, I’ve just finished my MSc in Computer Science at Oxford University, and am looking to work in Data Science / Machine Learning Engineering. I did my undergraduate degree at the University of St Andrews. My interests include Deep Learning, Reinforcement Learning, Graph Neural Networks and Solitaire card games. .",
          "url": "https://thecharlieblake.co.uk/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://thecharlieblake.co.uk/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}