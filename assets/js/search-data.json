{
  
    
        "post0": {
            "title": "Deep Learning Book Notes: Deep Feedforward Networks",
            "content": "Link to chapter: 2.6 Deep Feedforward Networks . (Note: I’ve only noted down chunks that were interesting / I haven’t already internalised. Fairly large chunks from the book are therefore not covered) . Gradient-Based Learning . Note that “stochastic gradient descent applied to non-convex loss functions has no […] convergence guarantee and is sensitive to the values of the initial parameters”. In other words, SGD-based methods simply find strong local optima, which depend on where we start looking. . Cost Functions . (Note: much of this section is also based on the previous chapter in the book) . A cost function enables us to calculate a scalar performance metric evaluating our model’s predictions over a dataset. We aim to optimise with respect to this metric, so we want to choose a cost function with a minimum point that represents the “best” performance for the model. . We typically think of the model’s functional form as fixed, but parameterised by some learned $ theta$. This reduces our optimisation problem to finding the right solution in parameter space, rather than function space. Thus we can frame the cost function as an evaluation of our model’s parameters. . We can think of our model as either outputting a single prediction, or as defining a conditional probability distribution: $p_{model}( mathbf{y} mid textbf{x} ; theta)$. We will consider the latter case first. We desire a cost function with a minimum at the point where our model is “best”, but how do we define this? . One common answer to this question is the maximum likelihood principle. This simply states that given a dataset (inputs and labels), the “best” model is the one that allocates the highest probability to the correct labels given the inputs. . The following steps show how we can describe this criterion using a cost function, denoted $J( theta)$: . We define the likelihood of the data as: $p_{model}( textbf{y}_0, dots textbf{y}_m mid textbf{x}_0, dots textbf{x}_m ; theta)$. | Assuming the data is i.i.d., we can factorise the joint distribution as: $ Pi^{m}{i=1}p{model}( textbf{y}_i mid textbf{x}_i ; theta)$. | Our criterion states that the “best” set of parameters should give the most probability to the training data. In mathematical terms, this means: $ theta_{best} = arg max_{ theta}{ Pi^{m}{i=1}p{model}( textbf{y}_i mid textbf{x}_i ; theta)}$. | As specified by our definition of the cost function, we need $ arg min_ theta J( theta) = theta_{best}$. | One form for $J( theta)$ which satisfies this is: $J( theta) = Pi^{m}{i=1}{-p{model}( textbf{y}_i mid textbf{x}_i ; theta)}$. | Long chains of multiplication can lead to problems such as numerical instability. Moving into log space solves this problem without changing $ theta_{best}$. This gives us our final form for $J( theta)$, the negative log-likelihood: | J(θ)=∑i=1m−log⁡pmodel(yi∣xi;θ)J( theta) = sum^{m}_{i=1}{- log p_{model}( textbf{y}_i mid textbf{x}_i ; theta)}J(θ)=i=1∑m​−logpmodel​(yi​∣xi​;θ) . We now have a criterion for our model’s parameters! This gives us something to tune the parameters with respect to, regardless of the choice of model. . There is another observation we can make to further justify our use of maximum likelihood. We can re-frame our formula for the NLL as an expectation in the following way: J(θ)=Ex,y∼pdata[−log⁡pmodel(y∣x;θ)]J( theta) = mathbb{E}_{ mathbf{x}, mathbf{y} sim p_{data}} left[- log p_{model}( mathbf{y} mid textbf{x} ; theta) right]J(θ)=Ex,y∼pdata​​[−logpmodel​(y∣x;θ)] This formulation is exactly the same as something we have seen before: the cross-entropy. This leads us to a neat conclusion: . Minimising the NLL is the same as minimising the cross-entropy of the model’s distribution relative to the data distribution. . Neat! . Note that this can also be framed as minimising the KL divergence between the two distributions, as the KL is simply $H(p_{data}, p_{model}) - H(p_{data})$ and the entropy term here is irrelevant for the optimisation. . This gives us three inter-related ways of motivating our decision to minimise the NLL: . It satisfies the maximum likelihood principle. | It minimises the cross-entropy of the model’s distribution relative to the data distribution. | It minimises the KL divergence from the model’s distribution to the data distribution. | Sigmoid Units for Bernoulli Output Distributions . Some good notes on this section can be found at: peterroelants.github.io/posts/cross-entropy-logistic . For tasks where the prediction is of a binary label, we can use our model to define a Bernoulli distribution over $y$ conditioned on $x$. The task of our network is to learn a conditional value for the distribution’s parameter $a$ (the final activation), which we can then use for prediction. . We have a particular requirement for this parameter: $a in [0, 1]$. To satisfy this, we must add a layer to the end of our network to bound the output $z$ (note: this value is sometimes called a logit). One common choice is the sigmoid function1. . The sigmoid function is defined as follows: . begin{align} a = sigma(z) = frac{e^z}{e^z + 1} = frac{1}{1 + e^{-z}} end{align} . We can use this to model $P(y = 1 mid x)$ (recalling that $z$ is a function of $x$), and then in accordance with the laws of probability we can take $P(y = 0 mid x) = 1 - P(y = 1 mid x)$ to give us our full distribution over labels. . Three interesting properties of this function are: . 1−σ(z)=σ(−z)σ′(z)=σ(z)(1−σ(z))=σ(z)σ(−z)∫σ(z)dz=log⁡(1+ez)=ζ(z)(softplus)1 - sigma(z) = sigma(-z) sigma^ prime(z) = sigma(z)(1- sigma(z)) = sigma(z) sigma(-z) int sigma(z)dz = log(1+e^z) = zeta(z) quad text{(softplus)}1−σ(z)=σ(−z)σ′(z)=σ(z)(1−σ(z))=σ(z)σ(−z)∫σ(z)dz=log(1+ez)=ζ(z)(softplus) . But why use this particular bounding function over any other form? Well, it turns out that if we assume a very simple linear model for the probability, this is what results. . We begin by modelling the unnormalised log probability, $ log tilde{P}(y mid x)$. This is a good place to start, as whereas $P(y mid x) in [0, 1]$, $ log tilde{P}(y mid x) in mathbb{R}$. The most simple model for our final layer is the linear model2: . log⁡P~(y∣x)=yz={z,y=10,y=0 log tilde{P}(y mid x) = yz = begin{cases} z, &amp; y=1 0, &amp; y=0 end{cases}logP~(y∣x)=yz={z,0,​y=1y=0​ . To convert this into an expression for the probability distribution we take the following steps: . &lt;span class=”katex-error” title=”ParseError: KaTeX parse error: No such environment: align at position 90: …ses} . begin{̲a̲l̲i̲g̲n̲}̲ P(y mid x) &amp;=…” style=”color:#cc0000”&gt; tilde{P}(y mid x) = e^{yz} = begin{cases} e^z, &amp; y=1 1, &amp; y=0 end{cases} . begin{align} P(y mid x) &amp;= begin{cases} frac{e^{z}}{1 + e^{z}}, &amp; y=1 frac{1}{1 + e^{z}}, &amp; y=0 end{cases} . &amp;= begin{cases} sigma(z), &amp; y=1 sigma(-z) = 1- sigma(z), &amp; y=0 end{cases} end{align}&lt;/span&gt; . Thus we have shown that the sigmoid activation function is the natural consequence of a linear model for our log probabilities. . We can use this form with the NLL defined in the previous section: . begin{align} J( theta) &amp;= - sum_{j=1}^m log P(y_j mid x) &amp;= sum_{j=1}^m begin{cases} log(1 + e^{-z_j}) = zeta(-z_j), &amp; y=1 log(1 + e^{z_j}) = zeta(z_j), &amp; y=0 end{cases} end{align} . Visualising the above softmax function, its curve looks like a smooth version of the ReLU function. To minimise these two cases (which is our objective for the cost function) we must therefore move to the positive and negative extremes for our respective cases. . Thus the consequence of using the sigmoid activation in combination with maximum likelihood is that our learning objective for the logits $z$ is to make the predictions for our 1 labels as positive as possible, and for our 0 labels as negative as possible. . This is pretty much what we would intuitively expect! It’s promising to see that ML in combination with a linear log probability model (i.e. sigmoid) leads to such a natural objective for our network. . Note that the above equation is not the form one typically sees the NLL/CE of a binary variable written in. More common (although perhaps less insightful) is the form: begin{align} J( theta) &amp;= - sum_{j=1}^m left[ y_j log(a_j) + (1-y_j) log(1-a_j) right] end{align} One practical consideration we also shouldn’t overlook here is how amenable this combination of cost function and final layer (/distribution) are to gradient-based optimisation. . What we really care about here is the degree to which the size of the gradient shrinks when the outputs of the layer are towards the extremes. We call this phenomenon saturation, and it leads to very slow learning in cases where we have predictions that are incorrect by a large amount3. . The derivative of the cost function with respect to $z$ are simply: . begin{align} frac{d}{dz} J( theta) &amp;= begin{cases} - sigma(-z) = sigma(z)-1, &amp; y=1 sigma(z), &amp; y=0 end{cases} &amp;= a - y end{align} . Take a moment to appreciate how wonderfully simple this is! . For the purpose of learning, the specific gradient values are ideal. In the case of a very wrong input for a positive label ($y=1, z to - infty$), we have $a = 0$ so the derivative tends to $-1$; for a very wrong negative label the derivative tends to $1$. . This is exactly the behaviour we want: large gradients for very wrong predictions (although not too large). Conversely, the gradient for good predictions tends to zero in both cases. Learning will only slow down for this layer when we get close to the right answer! . Softmax Units for Multinoulli Output Distributions . Some good notes on this section can also be found at: peterroelants.github.io/posts/cross-entropy-softmax . When we have $n$ output classes we instead use a Multinoulli distribution with $n$ parameters. The labels here are now $y in {1, dots, n}$. . To avoid an implicit assumption about numerically close classes being more similar, we model this using a network with $n$ outputs, $z_i, ; i in {1, dots, n}$. We add a final layer to convert each of these into a probability distribution over the associated class being the value of the label: $P(y=i mid x) = a_i = f(z_i)$. . We can think of our model’s final output as a vector that represents a probability distribution over labels. Note that this means we must also make sure to normalise the output values so that they sum to 1. . We use the same approach as in the binary-class case to model $f(z_i)$. We begin with a linear model for the log probability at each output: zi=log⁡P~(y=i∣x)z_i = log tilde{P}(y=i mid x)zi​=logP~(y=i∣x) Exponentiating and normalising gives: P(y=i∣x)=ai=softmax(z)i=ezi∑k=1nezkP(y=i mid x) = a_i = text{softmax}(z)_i = frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}}P(y=i∣x)=ai​=softmax(z)i​=∑k=1n​ezk​ezi​​ This $ text{softmax}(z)_i$ function is the generalisation of the sigmoid over a vector. It’s derivative is also similar (in fact, for the first case it’s the same). We find this using the quotient rule: begin{alignat}{1} text{if} ; ; i &amp;= j: frac{da_i}{dz_j} &amp;= frac{e^{z_i} sum_{k=1}^ne^{z_k} - e^{z_i}e^{z_j}}{ sum_{k=1}^ne^{z_k}} &amp;= frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} left(1- frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} right) &amp;= a_i(1-a_i) text{if} ; ; i &amp; ne j: frac{da_i}{dz_j} &amp;= frac{0 - e^{z_i}e^{z_j}}{ sum_{k=1}^ne^{z_k}} &amp;= - frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} frac{e^{z_j}}{ sum_{k=1}^ne^{z_k}} &amp;= -a_i a_j end{alignat} We can plug the softmax into the NLL to give: begin{align} J( theta) &amp;= - sum_{j=1}^m log P(y = y_j mid x) &amp;= - sum_{j=1}^m log a_{y_j} quad text{(we just ignore the other } a_{i ne y_j} text{)} &amp;= - sum_{j=1}^m log text{softmax}(z_{y_j}) &amp;= - sum_{j=1}^m left[z_{y_j} - log sum_{k=1}^n e^{z_k} right] end{align} One point worth noting is that this looks a bit simpler than the binary case - shouldn’t it be at least as complex? The reason for this is due to us having one parameter for each class here, whereas because of the probabilities summing to 1 we only really need $n-1$ parameters. We do this for the binary case, which makes the reasoning a little more complex. . Intuitively, we can understand this cost function as incentivising the correct output to increase and the rest to decrease. The second term also punishes the largest incorrect output the most, which is also desirable. . We can see exactly how learning progresses by calculating the gradient of the NLL. We do this over a single label $y$ for a single $z_i$: begin{align} frac{d}{dz_i} J( theta) &amp;= frac{d}{dz_i} left(- log a_y right) &amp;= - frac{1}{a_y} frac{da_y}{dz_i} &amp;= - frac{1}{a_y} begin{cases} a_i(1-a_i), &amp; i = y -a_ia_y, &amp; i ne y end{cases} &amp;= begin{cases} a_i - 1, &amp; i = y a_i, &amp; i ne y end{cases} &amp;= a_i - y^{(hot)}_i quad text{(where } y^{(hot)} text{ is the 1-hot encoded label vector)} end{align} Fantastic! This is exactly the same as the gradient in the binary-case, but we have it over each item of the output vector instead of the scalar we had before. Everything we deduced for that scenario works just the same here. . Wikipedia defines the sigmoid function as a general family of S-shaped curves, and refers to this particular function as the logistic function. &#8617; . | One useful feature of this form is that one case is constant. We will see when we normalise how this translates into a single output controlling the probabilities of both cases, as required for a Bernoulli distribution. &#8617; . | Sigmoid activation combined with MSE has exactly this problem. See how the left extreme of this graph (the derivative of the MSE) tends to 0, whereas in our case it tends to -1. &#8617; . |",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-2/feedforward/neural-network/2020/11/19/feedforward.html",
            "relUrl": "/deep-learning-book/part-2/feedforward/neural-network/2020/11/19/feedforward.html",
            "date": " • Nov 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning Book Notes: Probability & Information Theory",
            "content": "Link to chapter: 1.3 Probability and Information Theory . Common Probability Distributions . Bernoulli Distribution . Support: {$0, 1$} . Parameters: $0 leq p leq 1$ . PMF: . p(x)={1−px=0px=1p(x) = begin{cases} 1-p &amp; quad x=0 p &amp; quad x=1 end{cases}p(x)={1−pp​x=0x=1​ . CDF: . F(x)={0x&lt;01−p0≤x&lt;1px≥1F(x) = begin{cases} 0 &amp; quad x lt 0 1-p &amp; quad 0 leq x lt 1 p &amp; quad x ge 1 end{cases}F(x)=⎩⎪⎪⎨⎪⎪⎧​01−pp​x&lt;00≤x&lt;1x≥1​ . Mean: $p$ . Variance: $p(1-p)$ . Multinoulli / Categorical Distribution . Support: $x in$ {$1, dots, k$} . Parameters: . number of categories: $k &gt; 0$ . | event probabilities: $p_1, dots, p_k quad (p_i gt 0, sum{p_i} = 1)$ . | . PFM: . p(x=i)=pip(x = i) = p_ip(x=i)=pi​ . Normal / Gaussian Distribution . Support: $x in mathbb{R}$ . Parameters: $ mu in mathbb{R}, ; sigma^2 &gt; 0$ . PDF: . p(x)=1σ2πe−12(x−μσ)2p(x) = frac {1}{ sigma { sqrt {2 pi }}}e^{-{ frac {1}{2}} left({ frac {x- mu }{ sigma }} right)^{2}}p(x)=σ2π . ​1​e−21​(σx−μ​)2 . The CDF is more complex and cannot be expressed in terms of elementary functions. . The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. . The following gives an interesting Bayesian interpretation of the normal distribution: . Out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model. . Multivariate normal distribution, PDF: . (2π)−k2det⁡(Σ)−12 e−12(x−μ) ⁣TΣ−1(x−μ){ displaystyle (2 pi )^{-{ frac {k}{2}}} det({ boldsymbol { Sigma }})^{-{ frac {1}{2}}} ,e^{-{ frac {1}{2}}( mathbf {x} -{ boldsymbol { mu }})^{ !{ mathsf {T}}}{ boldsymbol { Sigma }}^{-1}( mathbf {x} -{ boldsymbol { mu }})}}(2π)−2k​det(Σ)−21​e−21​(x−μ)TΣ−1(x−μ) . Exponential Distribution . Support: $x in$ [$0, infty$) . Parameters: $ lambda &gt; 0$ . PDF: . λe−λx lambda e^{- lambda x}λe−λx . CDF: . 1−e−λx1 - e^{- lambda x}1−e−λx . One benefit of using this distribution is that it has a sharp peak at $x = 0$. . Laplace Distribution . Support: $x in mathbb{R}$ . Parameters: . location: $ mu in mathbb{R}$ | scale: $b &gt; 0$ | . PDF: . p(x)=12bexp⁡(−∣x−μ∣b)p(x) = { displaystyle { frac {1}{2b}} exp left(-{ frac {|x- mu |}{b}} right)}p(x)=2b1​exp(−b∣x−μ∣​) . CDF: . F(x)={12exp⁡(x−μb)if x≤μ1−12exp⁡(−x−μb)if x≥μF(x) = { displaystyle { begin{cases}{ frac {1}{2}} exp left({ frac {x- mu }{b}} right)&amp;{ text{if }}x leq mu [8pt]1-{ frac {1}{2}} exp left(-{ frac {x- mu }{b}} right)&amp;{ text{if }}x geq mu end{cases}}}F(x)=⎩⎪⎨⎪⎧​21​exp(bx−μ​)1−21​exp(−bx−μ​)​if x≤μif x≥μ​ . This is similar to the exponential distribution, but it allows us to place the peak anywhere we wish. . It is similar to the normal distribution too, but uses an absolute difference rather than the square. . Dirac Distribution . If we wish to specify that all the mass in a probability distribution clusters around a single point then we can use the Dirac delta function, $ delta(x)$, which is zero-valued everywhere except 0, yet integrates to 1 (this is a special mathematical object called a generalised function). . PDF: $p(x) = delta(x - mu)$ . Empirical Distribution . We can use the Dirac delta function with our training data, $x^{(1)}, dots, x^{(m)}$, to define the following PDF: . p(x)=1m∑i=1mδ(x−x(i))p(x) = frac{1}{m} sum^m_{i=1}{ delta(x - x^{(i)})}p(x)=m1​i=1∑m​δ(x−x(i)) . This concentrates all of the probability mass on the training data. In effect, this distribution represents the distribution that we sample from when we train a model on this dataset. . It is also the PDF that maximises the likelihood of the training data. . Information Theory . Information . The amount of information an event tells us, depends on its likelihood. Frequent events tell us little, while rare events tell us a lot. . Information theory gives us a measure called a nat, that quantifies how much information an event $x$ gives us. We denote this by $I(x)$. . Our requirements for such a function are that it satisfies the following: . An event with probability 1 has $I(x) = 0$ | The less likely an event, the more information it transmits | The information conveyed by independent events should be additive | . We therefore define a nat as follows: . I(x)=−log⁡P(x)I(x) = - log{P(x)}I(x)=−logP(x) . Here we use the natural logarithm. If base 2, is used this measurement is called shannons or bits. . Entropy . Moving to whole probability distributions, we define the expected information in an event sampled from a distribution as the Shannon entropy: . H(x)=Ex∼P[I(x)]H(x) = mathbb{E}_{x sim P}[I(x)]H(x)=Ex∼P​[I(x)] . Distributions that are nearer deterministic have lower entropies, and distributions that are nearer uniform have higher entropies. . When $x$ is continuous this is also known as differential entropy. . KL Divergence . If we want to compare the information in two probability distributions, we use the Kullback-Leibler divergence, which is the expected log probability ratio between the two distributions: . DKL(P∣∣Q)=Ex∼P[log⁡P(x)Q(x)]D_{KL}(P || Q) = mathbb{E}_{x sim P} left[ log frac{P(x)}{Q(x)} right]DKL​(P∣∣Q)=Ex∼P​[logQ(x)P(x)​] . The KL divergence is $0$ when $P$ and $Q$ are the same. . This is sometimes thought of as a measure of “distance” between the two distributions. However, this measure is not symmetric, so does not satisfy the typical requirements of distances. . To visualise the asymmetry, see figure 6.3 in the book. The key point here is that if we wish to minimise the kl: . From the perspective of Q: we want to make sure we have high probability whenever P has high probability (and if P has low probability, Q can be low or high) | From the perspective of P: we want to make sure we have low probability whenever Q has low probability (and if Q has high probability, P can be low or high) | . Cross-Entropy . A similar measure is the cross-entropy, which is defined as: . H(P,Q)=Ex∼P[−log⁡Q(x)]H(P, Q) = mathbb{E}_{x sim P}[- log{Q(x)}]H(P,Q)=Ex∼P​[−logQ(x)] . This measure can be thought of in the following way: . The cross entropy can be interpreted as the number of bits per message needed (on average) to encode events drawn from true distribution p, if using an optimal code for distribution q . Note that the cross entropy can be defined as the Shannon entropy of $P$ plus the KL divergence from $P$ to $Q$: . H(P,Q)=H(P)+DKL(P∣∣Q)H(P, Q) = H(P) + D_{KL}(P || Q)H(P,Q)=H(P)+DKL​(P∣∣Q) . Structured Probabilistic Models . Motivation . The number of parameters in a probability distribution over $n$ random variables is exponential in $n$. Hence, using a single probability distributions over a large number of random variables can be very inefficient. . If we can factorise joint probability distributions into chains of conditional distributions, we can greatly reduce the number of parameters and computational cost. . We call these structured probabilistic models or graphical models. . Directed Models . Given a graph $G$, we define the immediate parents of a node (as defined by the directed edges) as $Pa_G(x_i)$. We can then express the factorisation as follows: . p(x)=∏ip(xi∣PaG(xi))p( mathbf{x}) = prod_i{p(x_i|Pa_G(x_i))}p(x)=i∏​p(xi​∣PaG​(xi​)) . The graph itself effectively encodes a number of (mainly conditional) independence relations between random variables. Specifically, any two nodes are conditionally independent given the values of their parents. This is really what we’re exploiting to gain the efficiency speedup here. . Undirected Models . In undirected models we associate groups of nodes with a factor. . We define a clique $C^{(i)}$ as a set of nodes that are all connected to one-another. . Each clique in the model is then associated with a factor $ phi^{(i)}(C^{(i)})$. Note that these factors are simply non-negative functions; not probability distributions. . To obtain the full joint probability distribution, we then multiply and normalise: p(x)=1Z∏iϕ(i)(C(i))p( mathbf{x}) = frac{1}{Z} prod_i{ phi^{(i)}(C^{(i)})}p(x)=Z1​∏i​ϕ(i)(C(i)) where $Z$ is a normalising constant (i.e. the sum/integral of the probability over all outcomes). .",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-1/probability/information-theory/2020/11/11/prob.html",
            "relUrl": "/deep-learning-book/part-1/probability/information-theory/2020/11/11/prob.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning Book Notes: Linear Algebra",
            "content": "Link to chapter: 1.2 Linear Algebra . Matrix Multiplication . Key Terms . Span (of a matrix / set of vectors): the set of all vectors obtainable via linear combination . Column / row space: span of a matrix’s columns / rows . Linear independence: no vectors are in the span of any other group of vectors . Invertible matrix: square matrix with linearly independent columns . Motivation . Consider the “basic” matrix-vector multiplication equation: $Ax = b$, where $A$ is a fixed matrix and $x$ is a variable vector to be found. . For a fixed $b$, we care about the possible solutions (i.e. values of $x$): how many are there and what are they? . Considering the case where $b$ is arbitrary (i.e. considering what is true for all $b$) is perhaps more interesting, and can tell us a lot about $A$ and its properties. The key question is: does the equation have a solution for all $b$? . Solutions . In the case of a fixed $b$ the basic equation either has 0, 1 or $ infty$ many solutions. The authors provide a useful way of thinking about this: . To analyze how many solutions the equation has, think of the columns of $A$ as specifying diﬀerent directions we can travel in from the origin … then determine how many ways there are of reaching $b$. . In the case of an arbitrary $b$: . There is &gt;= 1 solution for all $b$ iff $A$ has a set of $m$ linearly independent columns. This is due to the column space of $A$ being all of $ mathbb{R}^{m}$. | A necessary (but not sufficient) condition here is $n gt m$ (at least as many columns as rows), otherwise the column space can’t span $ mathbb{R}^{m}$. | . | There is = 1 solution for all $b$ iff $A$ has exactly $m$ linearly independent columns. A necessary condition is therefore that m = n (i.e. $A$ is square). | If $A$ satisfies this condition we say it is invertible. | Note that a square matrix that is not invertible is called singular or degenerate. | . | . Why is this all useful? Consider… . Inverse Matrices . Think of $A$ as applying a transformation to a vector or matrix. . If the basic equation has one solution (i.e. $A$ is invertible) then this transformation can be reversed. This is often really useful! . This reversal can be expressed as the matrix inverse $A^{-1}$. . In practice computing $A^{-1}$ directly is often avoided as it can be numerically unstable, but this property is still very important. . Norms . Motivation . A norm is a function that gives us some measure of the distance from the origin to a vector or matrix. . Clearly this is a useful concept! . Definition . A norm is any function which satisfies the following three properties (for all $ alpha, x, y$): . point-separating: $f(x) = 0 implies x = 0$ . absolutely scalable: $f( alpha x) = | alpha|f(x)$ . triangle inequality: $f(x + y) &lt;= f(x) + f(y)$ . Vector Norms . The $L^p$ norm of $x$, often denoted by $||x||_p$ , is defined as: . ∣∣x∣∣p=(∑i∣xi∣p)1p||x||_p = left( sum_i |x_i|^p right)^{ frac{1}{p}} quad∣∣x∣∣p​=(i∑​∣xi​∣p)p1​ . where ($p in mathbb{R}, p ge 1$) . . The $L^1$ norm is called the Manhattan norm. . The $L^2$ norm is called the Euclidean norm. This is the standard norm and is commonly referred to without the subscript as simply $||x||$. The squared $L^2$ norm is also used in some contexts, which is simply $x^Tx$. . The $L^ infty$ norm is called the max norm. It is defined as $||x||_ infty = max_i{|x_i|}$. . Matrix Norms . (This is a much more complex field that we only touch on briefly here!) . We consider two analogous matrix norms for the $L^2$ vector norm. . In wider mathematics the spectral norm is often used. It can be useful in ML for analysing (among other things) exploding/vanishing gradients. It is defined as $A$’s largest singular value: $||A|| = sigma_{ max}{(A)}$ . However, most in most ML applications it is assumed the Frobenius norm is used. This is defined as: . ∥∥A∥∥F=∑i,jAi,j2 | |A | |_F = sqrt{ sum_{i,j}{A^2_{i,j}}}∥∥A∥∥F​=i,j∑​Ai,j2​ . ​ . Note that this is equivalent to: $||A||_F = sqrt{Tr(AA^T)}$ . Eigendecomposition . Key Terms . Unit vector: the $L^2$ norm = 1 . Orthogonal vectors: $x^Ty = 0$ . Orthonormal vectors: orthogonal unit vectors . Orthogonal matrix: rows &amp; columns are mutually orthonormal . Motivation . Decomposing matrices can help us learn about a matrix by breaking it down into its constituent parts. This can reveal useful properties about the matrix. . Eigendecomposition decomposes a matrix into eigenvectors and eigenvalues. . These tell us something about the directions and sizes of the transformation created when multiplying by the matrix. . Eigenvectors &amp; Eigenvalues . Vector $v$ and scalar $ lambda$ are a eigenvector-eigenvalue pair for square matrix $A$ iff: . $v neq mathbf{0}$ | $Av = lambda v$ | (Strictly speaking, here $v$ is a right eigenvector. A left eigenvector is such that $v^TA = lambda v^T$. We care primarily about right eigenvectors.) . If $v$ is an eigenvector it follows that any rescaled version of $v$ is also an eigenvector with the same eigenvalue. We typically use a scale such that we have a unit eigenvector. . If $A$ has $n$ independent eigenvectors we can combine them into the columns of a matrix, $V$, such that $AV = V diag( lambda)$. . The eigendecomposition of $A$ is then defined as: $A = V diag( lambda) V^{-1}$. . We are only guaranteed an eigendecomposition if $A$ is symmetric (and real-valued). In this case it is often denoted: . A=QΛQTA = Q Lambda Q^TA=QΛQT . Here the decomposition is guaranteed to be real-valued and $Q$ is orthogonal. . The decomposition may not be unique if two (independent) eigenvectors have the same eigenvalues. . Zero-valued eigenvalues exist iff $A$ is singular. . The Determinant . The determinant, noted $det(A)$, is a scalar that reflects how much multiplying by $A$ alters the volume of an object. . The determinant is calculated by taking the product of $A$’s eigenvalues. . $det(A) = 1 implies$ volume is preserved. . $det(A) = 0 implies$ space is contracted completely in at least one dimension. Thus volume = 0. . Singular Value Decomposition . Motivation . We want a decomposition for all real matrices. SVD provides this. . It is also key to computing the pseudoinverse (TODO), which enables us to find solutions to $Ax = b$ in all cases. . Singular Values &amp; Vectors . The SVD decomposes a matrix into: . A=UDVTA = UDV^TA=UDVT . where: . $U$ is an orthogonal $m times m$ matrix of left-singular vectors (columns) | $V$ is an orthogonal $n times n$ matrix of right-singular vectors (columns) | $D$ is a diagonal $m times n$ matrix of singular values (not necessarily square) | . These matrices are calculated as follows: . $U$ = the (unit norm) eigenvectors of $A^TA$ | $V$ = the (unit norm) eigenvectors of $AA^T$ | $D$ = the square roots of the eigenvalues of either (padded with zeroes if there aren’t enough). | . With the SVD we can calculate the… . Pseudoinverse . Motivation . There are either 0, 1 or $ infty$ solutions to $Ax = b$ . . The inverse, $A^-1$, allows us to find $x$ if there is a singular solution. . However, for the inverse to exist, $A$ must satisfy the properties for being invertible (exactly $m$ linearly independent columns). . The pseudoinverse gives us a method for finding the “best” $x$ for all possible values of $A$ and $b$. . The Moore-Penrose Pseudoinverse . We denote the pseudoinverse as $A^+$ and take our candidate for $x$ as follows: . x′=A+bx^ prime = A^+bx′=A+b . The pseudoinverse is defined as follows: . A+=VD+UTA^+ = VD^+U^TA+=VD+UT . where, . $V$ and $U$ are taken directly from the SVD, although the order here is reversed . | $D^+$ contains the reciprocals of the (non-zero) values in $D$, with a transpose applied to fix the dimensions. . | . When there are $ infty$ valid solutions, the pseudoinverse gives the value of $x^ prime$ with minimal $L^2$ norm. . When there are 0 valid solutions, the pseudoinverse gives the value of $x^ prime$ such that $Ax^ prime - b$ has minimal $L^2$ norm. . Principal Component Analysis . Motivation . We often encounter high dimensional data that can be compressed by projecting it to a lower dimensional space. . We can do this using linear algebra, by multiplying by a matrix (encoding) and multiplying by its inverse (decoding). . We want to find the matrix that minimises information loss when encoding. . Process . Suppose we have data $x^{(1)}, dots , x^{(m)}$ where $x^{(i)} in mathbb{R}^n$. . Encoding/decoding works as follows: . Encoder: $f( cdot)$ | Code: $c = f(x)$ | Decoder: $g( cdot)$ where $x approx g(c)$ | . We first define the decoder: $g(c) = Dc$ where $D in mathbb{R^{n times l}}$. . The columns of $D$ are orthogonal and have unit norm, but not square (so it’s not technically an orthogonal matrix). . The encoder is then defined: $f(x) = D^Tx$. . It can be shown this choice minimises $ |   | x - g(c) |   | _{2}^{2}$. (i.e. the gradient is zero when $c = D^Tx$) | . The entire process defines the PCA reconstruction operation: $r(x) = DD^Tx$. . Choice of Decoding Matrix . The only question that remains is what we should actually choose for the $l$ $n$-dimensional vectors that make up $D$. . It turns out the optimal choice here is the eigenvectors corresponsing to the $l$ largest eigenvalues of $X^TX$ (i.e. squared singular values of $X$). . The proof of this relies on the following: . To find $d^{(1)}$ we seek to minimise $|| x^{(1)} - r(x^{(1)}) ||_F^2$. | This can be shown to be equivalent to maximising $Tr left( frac{d^TX^TXd}{w^Tw} right)$ subject to $d^Td = 1$. | A standard result for a positive semidefinite matrix like $X^TX$ is that this fraction’s maximum possible value is equal to the largest eigenvalue of $X^TX$. | To reach this possible value, $d^{(1)}$ is required to equal the corresponding eigenvector. | This can be repeated for the subsequent values of $d$, which then correspond to the eigenvectors of the next smallest eigenvalues. |",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "relUrl": "/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "content": "Introduction . The paper focuses on the domain of visual object recognition. Interpreting high-dimensional pixel data is a difficult task, which requires learning complex relationships. . The authors outline two key components for addressing this: . Large datasets | Prior knowledge | These are addressed as follows: . Using the new ImageNet dataset, which has around $10^7$ images, versus $10^4/10^5$ for previous datasets. | Using the CNN architecture, whose inductive bias effectively encodes prior knowledge. | The paper makes the following contributions: . It demonstrates by far the best ImageNet results to date. | It presents a GPU-optimised implementation for CNNs that is key to scaling to larger datasets. | One key point which the authors perhaps don’t stress enough is as follows: . All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available. . This observation appears key to the enduring success of such methods. . The Dataset . ImageNet contains 15 million images of variable size, in roughly 22,000 categories. The images were taken from the web, and were labelled using Mechanical Turk. . The competition in question is the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), which has run since 2010. ILSVRC uses a subset of ImageNet with 1.2 million training images across 1000 categories. . On ImageNet the top-1 and top-5 error rates are typically reported, where ‘top-n’ denotes that the correct label is in the ‘n’ labels which a model gives the highest probability. . The only pre-processing is as follows: . Images are scaled to 256x256 | Each pixel is normalised according to the ‘mean activity’ across the dataset1. | Implementation . Architecture . The overall architecture contains 5 convolutional layers, followed by 3 fully-connected layers, and finally a softmax over the 1000 outputs. . The loss/objective is multinomial logistic regression. . Key Features . The authors describe their key implementation features from most to least important: . ReLU Nonlinearity . In 2012 the standard nonlinearity was $tanh$, but based on (Nair &amp; Hinton, 2010) they use $ReLU$ instead. . They demonstrate a 6x speedup on CIFAR-10 (a smaller dataset) when comparing the two. . Training on Multiple GPUs . They spread the neurons across two GPUs. Each is able to read/write to the other’s memory directly (i.e. without going through the main memory). . One trick they employ to make this work is to only have the GPUs communicate across certain layers. E.g. layer 2 is fully-connected to layer 3 across the two GPUs, but layer 3 is only fully-connected to layer 4 within the same GPU. This pattern is tuned as a hyperparameter (see the paper for an image detailing the final connectivity scheme, plus the layer/kernel dimensions). . This scheme reduces the error rates by (1.7%, 1.2%) compared with the single-GPU net. . Local Response Normalisation . One desirable property of ReLU is that is doesn’t require input normalisation to prevent saturation. However, the authors still find an advantage to be gained by using local normalisation. . Their scheme localises each activation in the network by a factor based on the mean squared activations of adjacent activations. This is not done between all layers, but only the first two. . This scheme reduces the error rates by (1.4%, 1.2%). . Overlapping Pooling . Previous CNN architectures did not overlap pooling layers, but here they do. This scheme reduces the error rates by (0.4%, 0.3%). . Reducing Overfitting . The model has 60 million parameters. The following techniques are designed to stop this large number of parameters from overfitting. . Data Augmentation . Augmentation is effectively computationally ‘free’, as it done on the CPU while waiting for the previous batch to train on the GPU. The following techniques are used: . Translations and horizontal reflections. | Altering pixel intensities (using a fairly complex PCA-based scheme to change the intensity and colour of the overall image). | Dropout . Ensemble learning is known to be a very successful way of improving model performance. The authors build on this by using the dropout technique introduced by (Hinton et al., 2012), which has a similar effect but using a single model. . For each batch, the output of each neuron is set to 0 with a probability of 0.5. The effectiveness of this is justified in the following way: . This technique reduces complex co-adaptations of neurons,since a neuron cannot rely on the presence of particular other neurons. . Dropout is applied to the first two fully-connected layers. . Weight Decay . The weight decay parameter is a small value: 0.0005. Nevertheless, this was found to be ‘important for the model to learn’. . Results . On ILSVRC-2010 the previous best result was (45.7%, 25.7%). The CNN model achieves (37.5%, 17.0%). . On ILSVRC-2012 the second-best result was (. , 26.2%). The CNN model achieves (. , 18.2%) **, and using an ensemble of 7 CNNs where 2 are pre-trained on ILSVRC-2011 achieves **(. , 15.3%). . Discussion . The authors highlight the following points of note: . CNNs are a highly effective model architecture. | Their depth is key here, as removing even a single layer damages performance. | Only supervised learning is necessary to achieve these results. | They further speculate that: . Unsupervised pre-training could help significantly, especially in the case where the dataset stays the same size but the number of model parameters increases. | They are still several orders of magnitude away from the human visual system. | These networks could be especially powerful for video sequences, where the temporal structure provides further useful information. | My Thoughts . Why It Works . A few points seem notable in terms of why these methods have proved so effective. I will give these in order of significance: . These methods are designed to scale with compute and data: this presumably paved the way for CNNs to make such progress in computer vision. | The size of ImageNet: this dataset, which was relatively new at this point, is several orders of magnitude larger than previous datasets. This gave huge scope for a big breakthrough. | Hardware made this possible: training such a large model was only really possible because of GPUs. They again rode this wave to great effect. | Some neat tricks: ReLU, dropout, data augmentation, normalisation and weight decay have become central tools. Each of these knocked a small chunk off the error rate. Such tricks are very important and easily overlooked. | Big hyperparameter search: although exact details aren’t reported, it sounds like the authors were able to throw a lot of compute at getting tuning many different aspects of the model, even down to the GPU communication. It should not be underestimated the extent to which throwing resources at this problem can yield substantial performance increases. | The Paper Itself . The paper is well-structured, to-the-point and clear. . One criticism I have is with a lack of discussion of the methods they are comparing against though, which would have helped contextualise/justify their approach. I also would like more assurance that they are making a fair comparison with other methods. I’m not certain, for instance how the methods they compare against would fare if given an equal amount of compute (although given the success of CNNs here and subsequently, I would almost certainly expect them to do better under such conditions). . I would also have loved to see a proper table of ablations in an appendix (plus details of hyperparameter search). They do have some great ablation details in the body of the paper, but it would be good to see them all done thoroughly in one place. . Thoughts for Further Reading . In a rough order of priority: . The original paper on dropout by (Hinton et al., 2012). | The original paper on ReLU by (Nair &amp; Hinton, 2010). | Subsequent papers which improve on ILSVRC. | Other subsequent key papers in CV. | As suggested in the discussion, any work done using CNNs for video data. | It’s not clear to me what is meant by ‘mean activity’ here. Does activity mean pixel value? And is this per-pixel or across all of them? &#8617; . |",
            "url": "https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html",
            "relUrl": "/paper-notes/krizhevsky2017imagenet.html",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://thecharlieblake.co.uk/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://thecharlieblake.co.uk/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "As of October 2020, I’ve just finished my MSc in Computer Science at Oxford University, and am looking to work in Data Science / Machine Learning Engineering. I did my undergraduate degree at the University of St Andrews. My interests include Deep Learning, Reinforcement Learning, Graph Neural Networks and Solitaire card games. .",
          "url": "https://thecharlieblake.co.uk/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://thecharlieblake.co.uk/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}