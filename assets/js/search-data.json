{
  
    
        "post0": {
            "title": "Bare-Bones Backpropagation",
            "content": "Making Backprop Simple . The first few times I came across backpropagation I struggled to get a feel for what was going on. It&#39;s not just enough to follow the equations - I couldn&#39;t visualise the operations and updates in a nice, clean way. . If someone had shown me then how it&#39;s really just a generalisation of our old friend the chain rule--and barely any more complex--then I think it would have helped me a lot. But instead I got bogged down in the matrix notation, and figuring out what dimension went with what, and lost sight of what was really going on. . So this is a simple-as-possible backprop implementation. I don&#39;t go into the maths here. I assume the reader already knows what&#39;s going on in theory, but doesn&#39;t have a great feel for what happens in practice, when we have batches and layers and thesors and so on. . This can also serve as a reference for how to implement this from scratch in the nicest way. Enjoy! . Setup . First things first, there&#39;s some setup to do. This isn&#39;t a tutorial on data loading, so I&#39;m just going to paste some code for loading up our dataset and we can ignore the details. The only thing worth noting is that we&#39;ll be using the classic MNIST dataset: . import math import torch import torchvision from torchvision.datasets import MNIST from torch.utils.data import DataLoader from torch.nn.functional import one_hot from functools import reduce import altair as alt import pandas as pd batch_sz = 64 train = DataLoader(MNIST(&#39;data/&#39;, train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), lambda x: torch.flatten(x), ]) ), batch_size=batch_sz, shuffle=True) . . This train dataloader can be iterated over and returns minibatches of shape (batch__sz, input_dim). . In our case, these values are (64, 28*28=784) . Layer-by-layer . We&#39;ll construct our neural network by making classes for each layer. We&#39;ll use a standard setup of: linear layer, ReLU, linear layer, softmax; plus a cross-entropy loss. . For each layer class we require two methods: . __call__(self, x): implements the forward pass. __call__ allows us to feed an input through the layer by treating the initialised layer object as a function. For example: relu_layer = ReLU(); output = relu_layer(input). | . bp_input(self, grad): implements the backward pass, allowing us to backpropagate the gradient vector through the layer. The grad parameter is a matrix of partial derivatives of the loss, with respect to the data sent from the given layer to the next. As such, it is a (batch_sz, out) matrix. The job of bp_input is to return a (batch_sz, in) matrix to be sent to the next layer by multiplying grad by the derivative of the forward pass with respect to the input (or an equivalent operation). | . There are two other methods we sometimes wish to implement for different layers: . __init__(self, ...): initialises the layer, e.g. weights. | . bp_param(self, grad): the &quot;final stop&quot; of the backwards pass. Only applicable for layers with trainable weights. Similar to bp_input, but calculates the derivative with respect to the weights of the layer. Should return a matrix with the same shape as the weights (self.W) to be updated. | . . Important: The key point to recall when visualising this is that when we have a batch dimension it is always the first dimension. For both the forward and backward pass. This makes everything much simpler! . Linear Layer . Let&#39;s start with the linear layer. We do the following: . We start by initialising the weights (in this case using the Xavier initialisation). | We then implement the call method. Rather than adding an explicit bias, we append a vector of ones to the layer&#39;s input (this is equivalent, and makes backprop simpler). | Backpropagation with respect to the input is just right multiplication by the transpose of the weight matrix (adjusted to remove the added 1s column) | Backpropagation with respect to the output is left multiplication by the transpose of the input matrix. | class LinearLayer: def __init__(self, in_sz, out_sz): self.W = self._xavier_init(in_sz + 1, out_sz) # (in+1, out) def _xavier_init(self, i, o): return torch.Tensor(i, o).uniform_(-1, 1) * math.sqrt(6./(i + o)) def __call__(self, X): # (batch_sz, in) self.X = torch.cat([X, torch.ones(X.shape[0], 1)], dim=1) # (batch_sz, in+1) return self.X @ self.W # (batch_sz, in+1) @ (in+1, out) = (batch_sz, out) def bp_input(self, grad): return (grad @ self.W.T)[:,:-1] # (batch_sz, out) @ (out, in) = (batch_sz, in) def bp_param(self, grad): return self.X.T @ grad # (in+1, batch_sz) @ (batch_sz, out) = (in+1, out) . ReLU Layer . Some non-linearity is a must! Bring on the RelU function. The implementation is pretty obvious here. clamp() is doing all the work. . class ReLU: def __call__(self, X): self.X = X return X.clamp(min=0) # (batch_sz, in) def bp_input(self, grad): return grad * (self.X &gt; 0).float() # (batch_sz, in) . Softmax &amp; Cross Entropy Loss . What? Both at once, why would you do this?? . This is quite common, and I can justify it in two ways: . This layer-loss combination often go together, so why not put them all in one layer? This saves us from having to do two separate forward and backward propagation steps. | I won&#39;t prove it here, but it turns out that the derivative of the loss with respect to the input to the softmax, is much simpler than the two intermediate derivative operations, and bypasses the numerical stability issues that arise when we do the exponential and the logarithm. Phew! | The downside here is that is we&#39;re just doing inference then we only want the softmax output. But for the purposes of this tutorial we only really care about training. So this will do just fine! . There&#39;s a trick in the second line of the softmax implementation: it turns out subtracting the argmax from the softmax input keeps the output the same, but the intermediate values are more numerically stable. How neat! . Finally, we examine the backprop step. It&#39;s so simple! Our starting grad for backprop (the initial grad value passed in is just the ones vector) is the difference in our predicted output vector and the actual one-hot encoded label. This is so intuitive and wonderful. . Tip: This is exactly the same derivative as when we don&#8217;t use a softmax layer and apply an MSE loss (i.e. the regression case). We can thus think of softmax + cross entropy as a way of getting to the same underlying backprop, but in the classification case. . class SoftmaxCrossEntropyLoss: # (batch_sz, in=out) for all dims in this layer def __call__(self, X, Y): self.Y = Y self.Y_prob = self._softmax(X) self.loss = self._cross_entropy_loss(Y, self.Y_prob) return self.Y_prob, self.loss def _softmax(self, X): self.X = X X_adj = X - X.amax(dim=1, keepdim=True) exps = torch.exp(X_adj) return exps / exps.sum(axis=1, keepdim=True) def _cross_entropy_loss(self, Y, Y_prob): return (-Y * torch.log(Y_prob)).sum(axis=1).mean() def bp_input(self, grad): return (self.Y_prob - self.Y) * grad . Putting it all together . Let&#39;s bring these layers together in a class: our NeuralNet implementation. . The evaluate() function does two things. Firstly, it runs the forward pass by chaining the __call__() functions, to generate label probabilities. Secondly, it uses the labels passed to it to calculate the loss and percentage correctly predicted. . Note: for this simplified example we don&#8217;t have a pure inference function, but we could add one with a small change to SoftmaxCrossEntropyLoss. The gradient_descent() function then gets the matrix of updates for each weight matrix and applies the update. The key bit here is how backprop() works. Going backwards through the computation graph we chain the backprop with respect to input methods. Then for each weighted layer we want to update, we apply the backprop with respect to prameters method to the relevant gradient vector. . class NeuralNet: def __init__(self, input_size=28*28, hidden_size=32, output_size=10, alpha=0.001): self.alpha = alpha self.z1 = LinearLayer(input_size, hidden_size) self.a1 = ReLU() self.z2 = LinearLayer(hidden_size, output_size) self.loss = SoftmaxCrossEntropyLoss() def evaluate(self, X, Y): out = self.z2(self.a1(self.z1(X))) correct = torch.eq(out.argmax(axis=1), Y).double().mean() Y_prob, loss = self.loss(out, one_hot(Y, 10)) return Y_prob, correct, loss def gradient_descent(self): delta_W1, delta_W2 = self.backprop() self.z1.W -= self.alpha * delta_W1 self.z2.W -= self.alpha * delta_W2 def backprop(self): d_out = torch.ones(*self.loss.Y.shape) d_z2 = self.loss.bp_input(d_out) d_a1 = self.z2.bp_input(d_z2) d_z1 = self.a1.bp_input(d_a1) d_w2 = self.z2.bp_param(d_z2) d_w1 = self.z1.bp_param(d_z1) return d_w1, d_w2 . Training the model . We&#39;re almost there! I won&#39;t go into this bit too much because this tutorial isn&#39;t about training loops, but it&#39;s all very standard here. . We break the training data into minibatches and train on them over 10 epochs. The evaluation metrics plotted are those recorded during regular training. . Warning: these results are only on the training set! In practice we should always plot performance on a test set, but we don&#8217;t want to clutter the tutorial with this extra detail. . model = NeuralNet() stats = {&#39;correct&#39;: [], &#39;loss&#39;: [], &#39;epoch&#39;: []} for epoch in range(10): correct, loss = 0, 0 for i, (X, y) in enumerate(train): y_prob, batch_correct, batch_loss = model.evaluate(X, y) model.gradient_descent() correct += batch_correct / len(train) loss += batch_loss / len(train) stats[&#39;correct&#39;].append(correct.item()) stats[&#39;loss&#39;].append(loss.item()) stats[&#39;epoch&#39;].append(epoch) print(f&#39;epoch: {epoch} | correct: {correct:.2f}, loss: {loss:.2f}&#39;) base = alt.Chart(pd.DataFrame.from_dict(stats)).mark_line() .encode(alt.X(&#39;epoch&#39;, axis=alt.Axis(title=&#39;epoch&#39;))) line1 = base.mark_line(stroke=&#39;#5276A7&#39;, interpolate=&#39;monotone&#39;) .encode(alt.Y(&#39;loss&#39; , axis=alt.Axis(title=&#39;Loss&#39; , titleColor=&#39;#5276A7&#39;), scale=alt.Scale(domain=[0.0, max(stats[&#39;loss&#39; ])])), tooltip=&#39;loss&#39; ) line2 = base.mark_line(stroke=&#39;#57A44C&#39;, interpolate=&#39;monotone&#39;) .encode(alt.Y(&#39;correct&#39;, axis=alt.Axis(title=&#39;Correct&#39;, titleColor=&#39;#57A44C&#39;), scale=alt.Scale(domain=[min(stats[&#39;correct&#39;]), 1.0])), tooltip=&#39;correct&#39;) alt.layer(line1, line2).resolve_scale(y = &#39;independent&#39;) . . epoch: 0 | correct: 0.87, loss: 0.47 epoch: 1 | correct: 0.92, loss: 0.28 epoch: 2 | correct: 0.93, loss: 0.23 epoch: 3 | correct: 0.94, loss: 0.20 epoch: 4 | correct: 0.95, loss: 0.18 epoch: 5 | correct: 0.95, loss: 0.16 epoch: 6 | correct: 0.96, loss: 0.15 epoch: 7 | correct: 0.96, loss: 0.14 epoch: 8 | correct: 0.96, loss: 0.13 epoch: 9 | correct: 0.96, loss: 0.12 . And our results are great! After 10 epochs I&#39;m getting a whopping 97% correct. . Given we&#39;ve implemented this all from scratch, backprop included, to get these results using only 4 notebook cells worth of code is a testament to how simple backprop really is! .",
            "url": "https://thecharlieblake.co.uk/2020/12/23/simple-backprop.html",
            "relUrl": "/2020/12/23/simple-backprop.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Measure the Difficulty of a Klondike Deal",
            "content": "Preamble . In 2018 Ian Gent and I worked on a project to solve solitaire card games. We wrote a paper together and produced a rather neat solver called Solvitaire. We managed to prove the win rate for the most popular solitaire game, Klondike, to within a very tight bound (81.956% ± 0.096% to be precise!) and found a lot of other win rates for many other games. . Several people have asked me recently how one might measure the difficulty of a particular deal/layout in standard Klondike. I.e. given some starting setup for the game, is there any way I can automatically get a good estimate for how long it will take the average player to solve, without having to actually play the game themselves? . The answer to this is, of course, sort of! . Someone who I’ve been in correspondence with recently posted this question on the board games stack exchange and I decided to reply. You can see the post at this link. Because my post was rather long I’ve decided to reproduce it here too: . . From a human perspective I suppose one’s interpretation of difficulty is quite subjective: what one person might find difficult another may find easy. If, for our purposes, we use a rough definition along the lines of, “how much time the average player spends on a layout”, then there are a few interesting things we can say about the relationship between a layout’s difficulty and its general features. . As your first point suggests, the solvability of the layout (i.e. does a sequence of legal moves exist that results in a win) is obviously key. Let’s consider this case first: . Unsolvable Layouts . If a layout is not solvable then in one sense it’s infinitely difficult. However, even for unsolvable layouts, the depth of the search tree can reflect a kind of difficulty. . In the extreme case, if to begin with there are no legal moves available in the tableau (main cards) or the stock, then although the layout isn’t solvable, in a sense it is easy because you can immediately give up. Alternatively, a layout may have a huge number of promising moves, only to turn out to be unsolvable in the end. . Using a tool like Solvitaire (full disclosure: I am one of the authors of Solvitaire) can identify unsolvable layouts and record the number of unique states that needed to be searched to prove that there’s no solution (often a very large number of states!). Another good solver is Klondike-Solver, although I’m not aware of what metrics it reports. . This isn’t a perfect measure by any means. Humans aren’t computers, and our brains probably aren’t using depth-first search in the way a solver might. Without looking at human gameplay data though, this is probably the best heuristic of difficulty for unsolvable layouts. . Solvable Layouts . As was the case for unsolvable layouts, if one has access to human gameplay data then that’s almost certainly going to be the best source of understanding how difficult humans will find a layout. . Let’s assume though, that we don’t have this data available, but we do have a computerised solver. Just looking at the static cards in the starting layout is unlikely to tell us much, but by running the solver we can deduce more about a deal’s difficulty (for a human). . Here are some heuristics one could use instead: . Size of Search Tree . For a computer this is important, but in the solvable case it may not tell us much about human play. For instance, the search tree for a layout could be huge, but have one solution that’s very obvious to a human. We would not want to classify such a layout as difficult just because it has a large search tree. . Depth of Shallowest / Best Solution . Based on the above logic, the depth of the shallowest (i.e. “best”) solution in the search tree is perhaps a better measure. However, this still may not be ideal. We might label a layout as easy because it has a shallow solution, but if this solution is extremely hard for a human to spot then our heuristic will be misleading. . Number of different solutions . This heuristic could be quite accurate, although it’s hard to say. Having a lot of different solutions in the search tree would suggest a human player is likely to come across a solution sooner, but there is no guarantee. This would certainly be something worth exploring further. . Number of states searched until first solution . This is probably the most obvious heuristic: how many states did it take before the solver came across the first solution? However, this measure can be deeply flawed. There is no guarantee that computerised solvers search in anything like the kind of order a human would search in (Solvitaire certainly doesn’t). . Say move “A” obviously leads to a solution in a few moves, whereas move “B” leads in a very different direction. There is no guarantee the solver picks “A” first; it might instead pick “B” and try thousands or even millions more moves before it backtracks to the point where it thinks to try “A”. . To make this kind of method work one needs a policy for which moves to try. For Solvitaire, which just wants to explore all possible moves, this doesn’t matter, but for determining difficulty this is very important! If one could come up with a policy which reflected closely how an average human would play, then the number of states searched until the first solution would be an excellent metric. But coming up with such a metric is hard, complex work. . What Is A Move: Dominances and K+ . In all of these discussions we’ve talked about moves and states. But how we define a move in Klondike is actually not trivial. . Firstly, consider the stock. The standard rules require we turn over 3 cards in one go. A consequence of this is that at different times there are different groups of cards that are effectively “available” to us. . Bjarnason et al. use a solver which can move any of the available stock cards into the tableau in a single move (they call this the K+ representation). For a human, this is like saying “I remember there’s a 5H in there, so I’ll just loop back over the stock to get it”. From the point of considering difficulty, one may wish to consider this sort of thing to be a single move, rather than several separate moves. . Finally, in our paper on Solvitaire we consider something called dominances. These are cases in which we can prove that one of the available moves is guaranteed to be a good move. For instance, sometimes it can clearly be shown that “putting up” say an ace, is always the right thing to do. This is often obvious to a human player and one might not wish to count this as a move (Solvitaire doesn’t count this as an extra move). . In Summary . One must first consider solvability | Then if a layout is solvable, one can begin with easier, but probably less accurate methods like the number of possible solutions, or the depth of the best solution | Finally, and only if one can craft a good (human-like) policy for moves to try at each search step, a better approach may be to measure the number of states the solver searches until the fist solution | .",
            "url": "https://thecharlieblake.co.uk/markdown/2020/11/27/klondike-difficulty.html",
            "relUrl": "/markdown/2020/11/27/klondike-difficulty.html",
            "date": " • Nov 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning Book Notes: Deep Feedforward Networks",
            "content": "Link to chapter: 6 Deep Feedforward Networks . (Note: I’ve only noted down sections that were interesting / I haven’t already internalised. Fairly large chunks from the book are therefore not covered) . Gradient-Based Learning . One thing worth noting before we begin is that, in the words of the authors: . stochastic gradient descent applied to non-convex loss functions has no […] convergence guarantee and is sensitive to the values of the initial parameters . In other words, SGD-based methods simply find strong local optima, which depend on where we start looking. These methods are not perfect and have a limited theoretical justification for the kinds of onn-convex problems we typically use them for (Geoff Hinton thinks we should start again from scratch), although empirically they seem to work well. . Cost Functions . (Note: much of this section is based on the previous chapter in the book) . A cost function enables us to calculate a scalar performance metric evaluating our model’s predictions over a dataset. We aim to optimise with respect to this metric, so we want to choose a cost function with a minimum point that represents the “best” performance for the model. . We typically think of the model’s functional form as fixed, but parameterised by some learned $ theta$. This reduces our optimisation problem to finding the right solution in parameter space, rather than function space. Thus we can frame the cost function as an evaluation of our model’s parameters. . We can think of our model as either outputting a single prediction, or as defining a conditional probability distribution: $p_{model}( mathbf{y} mid textbf{x} ; theta)$. We will consider the latter case first. We desire a cost function with a minimum at the point where our model is “best”, but how do we define this? . One common answer to this question is the maximum likelihood principle. This simply states that given a dataset (inputs and labels), the “best” model is the one that allocates the highest probability to the correct labels given the inputs. . The following steps show how we can describe this criterion using a cost function, denoted $J( theta)$: . We define the likelihood of the data as: $p_{model}( textbf{y}_0, dots textbf{y}_m mid textbf{x}_0, dots textbf{x}_m ; theta)$. | Assuming the data is i.i.d., we can factorise the joint distribution as: $ Pi_{i=1}^m p_{model}( textbf{y}_i mid textbf{x}_i ; theta)$. | Our criterion states that the “best” set of parameters should give the most probability to the training data. In mathematical terms, this means: $ theta_{best} = arg max_{ theta}{ Pi_{i=1}^m p_{model}( textbf{y}_i mid textbf{x}_i ; theta)}$. | As specified by our definition of the cost function, we need $ arg min_ theta J( theta) = theta_{best}$. | One form for $J( theta)$ which satisfies this is: $J( theta) = Pi_{i=1}^m {-p_{model}( textbf{y}_i mid textbf{x}_i ; theta)}$. | Long chains of multiplication can lead to problems such as numerical instability. Moving into log space solves this problem without changing $ theta_{best}$. This gives us our final form for $J( theta)$, the negative log-likelihood: | J(θ)=∑i=1m−log⁡pmodel(yi∣xi;θ)J( theta) = sum^{m}_{i=1}{- log p_{model}( textbf{y}_i mid textbf{x}_i ; theta)}J(θ)=i=1∑m​−logpmodel​(yi​∣xi​;θ) . We now have a criterion for our model’s parameters! This gives us something to tune the parameters with respect to, regardless of the choice of model. . There is another observation we can make to further justify our use of maximum likelihood. We can re-frame our formula for the NLL as an expectation in the following way: J(θ)=Ex,y∼pdata[−log⁡pmodel(y∣x;θ)]J( theta) = mathbb{E}_{ mathbf{x}, mathbf{y} sim p_{data}} left[- log p_{model}( mathbf{y} mid textbf{x} ; theta) right]J(θ)=Ex,y∼pdata​​[−logpmodel​(y∣x;θ)] This formulation is exactly the same as something we have seen before: the cross-entropy. This leads us to a neat conclusion: . Minimising the NLL is the same as minimising the cross-entropy of the model’s distribution relative to the data distribution. . Neat! . Note that this can also be framed as minimising the KL divergence between the two distributions, as the KL is simply $H(p_{data}, p_{model}) - H(p_{data})$ and the entropy term here is irrelevant for the optimisation. . This gives us three inter-related ways of motivating our decision to minimise the NLL: . It satisfies the maximum likelihood principle. | It minimises the cross-entropy of the model’s distribution relative to the data distribution. | It minimises the KL divergence from the model’s distribution to the data distribution. | Sigmoid Units for Bernoulli Output Distributions . Some good notes on this section can be found at: peterroelants.github.io/posts/cross-entropy-logistic . For tasks where the prediction is of a binary label, we can use our model to define a Bernoulli distribution over $y$ conditioned on $x$. The task of our network is to learn a conditional value for the distribution’s parameter $a$ (the final activation), which we can then use for prediction. . We have a particular requirement for this parameter: $a in [0, 1]$. To satisfy this, we must add a layer to the end of our network to bound the output $z$ (note: this value is sometimes called a logit). One common choice is the sigmoid function1. . The sigmoid function is defined as follows: . a=σ(z)=ezez+1=11+e−za = sigma(z) = frac{e^z}{e^z + 1} = frac{1}{1 + e^{-z}}a=σ(z)=ez+1ez​=1+e−z1​ . We can use this to model $P(y = 1 mid x)$ (recalling that $z$ is a function of $x$), and then in accordance with the laws of probability we can take $P(y = 0 mid x) = 1 - P(y = 1 mid x)$ to give us our full distribution over labels. . Three interesting properties of this function are: . 1−σ(z)=σ(−z)σ′(z)=σ(z)(1−σ(z))=σ(z)σ(−z)∫σ(z)dz=log⁡(1+ez)=ζ(z)(softplus)1 - sigma(z) = sigma(-z) sigma^ prime(z) = sigma(z)(1- sigma(z)) = sigma(z) sigma(-z) int sigma(z)dz = log(1+e^z) = zeta(z) quad text{(softplus)}1−σ(z)=σ(−z)σ′(z)=σ(z)(1−σ(z))=σ(z)σ(−z)∫σ(z)dz=log(1+ez)=ζ(z)(softplus) . But why use this particular bounding function over any other form? Well, it turns out that if we assume a very simple linear model for the probability, this is what results. . We begin by modelling the unnormalised log probability, $ log tilde{P}(y mid x)$. This is a good place to start, as whereas $P(y mid x) in [0, 1]$, $ log tilde{P}(y mid x) in mathbb{R}$. The most simple model for our final layer is the linear model2: . log⁡P~(y∣x)=yz={z,y=10,y=0 log tilde{P}(y mid x) = yz = begin{cases} z, &amp; y=1 0, &amp; y=0 end{cases}logP~(y∣x)=yz={z,0,​y=1y=0​ . To convert this into an expression for the probability distribution we take the following steps: . P~(y∣x)=eyz={ez,y=11,y=0P(y∣x)={ez1+ez,y=111+ez,y=0P(y∣x)={σ(z),y=1σ(−z)=1−σ(z),y=0 tilde{P}(y mid x) = e^{yz} = begin{cases} e^z, &amp; y=1 1, &amp; y=0 end{cases} . P(y mid x) = begin{cases} frac{e^{z}}{1 + e^{z}}, &amp; y=1 frac{1}{1 + e^{z}}, &amp; y=0 end{cases} . P(y mid x) = begin{cases} sigma(z), &amp; y=1 sigma(-z) = 1- sigma(z), &amp; y=0 end{cases}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;P~(y∣x)=eyz={ez,1,​y=1y=0​P(y∣x)={1+ezez​,1+ez1​,​y=1y=0​P(y∣x)={σ(z),σ(−z)=1−σ(z),​y=1y=0​&lt;/span&gt;&lt;/span&gt; . Thus we have shown that the sigmoid activation function is the natural consequence of a linear model for our log probabilities. . We can use this form with the NLL defined in the previous section: . J(θ)=−∑j=1mlog⁡P(yj∣x)J(θ)=∑j=1m{log⁡(1+e−zj)=ζ(−zj),y=1log⁡(1+ezj)=ζ(zj),y=0J( theta) = - sum_{j=1}^m log P(y_j mid x) J( theta) = sum_{j=1}^m begin{cases} log(1 + e^{-z_j}) = zeta(-z_j), &amp; y=1 log(1 + e^{z_j}) = zeta(z_j), &amp; y=0 end{cases}J(θ)=−j=1∑m​logP(yj​∣x)J(θ)=j=1∑m​{log(1+e−zj​)=ζ(−zj​),log(1+ezj​)=ζ(zj​),​y=1y=0​ . Visualising the above softmax function, its curve looks like a smooth version of the ReLU function. To minimise these two cases (which is our objective for the cost function) we must therefore move to the positive and negative extremes for our respective cases. . Thus the consequence of using the sigmoid activation in combination with maximum likelihood is that our learning objective for the logits $z$ is to make the predictions for our 1 labels as positive as possible, and for our 0 labels as negative as possible. . This is pretty much what we would intuitively expect! It’s promising to see that ML in combination with a linear log probability model (i.e. sigmoid) leads to such a natural objective for our network. . Note that the above equation is not the form one typically sees the NLL/CE of a binary variable written in. More common (although perhaps less insightful) is the form: . J(θ)=−∑j=1m[yjlog⁡(aj)+(1−yj)log⁡(1−aj)]J( theta) = - sum_{j=1}^m left[ y_j log(a_j) + (1-y_j) log(1-a_j) right]J(θ)=−j=1∑m​[yj​log(aj​)+(1−yj​)log(1−aj​)] . One practical consideration we also shouldn’t overlook here is how amenable this combination of cost function and final layer (/distribution) are to gradient-based optimisation. . What we really care about here is the degree to which the size of the gradient shrinks when the outputs of the layer are towards the extremes. We call this phenomenon saturation, and it leads to very slow learning in cases where we have predictions that are incorrect by a large amount3. . The derivative of the cost function with respect to $z$ are simply: . ddzJ(θ)={−σ(−z)=σ(z)−1,y=1σ(z),y=0ddzJ(θ)=a−y frac{d}{dz} J( theta) = begin{cases} - sigma(-z) = sigma(z)-1, &amp; y=1 sigma(z), &amp; y=0 end{cases} frac{d}{dz} J( theta) = a - ydzd​J(θ)={−σ(−z)=σ(z)−1,σ(z),​y=1y=0​dzd​J(θ)=a−y . Take a moment to appreciate how wonderfully simple this is! . For the purpose of learning, the specific gradient values are ideal. In the case of a very wrong input for a positive label ($y=1, z to - infty$), we have $a = 0$ so the derivative tends to $-1$; for a very wrong negative label the derivative tends to $1$. . This is exactly the behaviour we want: large gradients for very wrong predictions (although not too large). Conversely, the gradient for good predictions tends to zero in both cases. Learning will only slow down for this layer when we get close to the right answer! . Softmax Units for Multinoulli Output Distributions . Some good notes on this section can also be found at: peterroelants.github.io/posts/cross-entropy-softmax . When we have $n$ output classes we instead use a Multinoulli distribution with $n$ parameters. The labels here are now $y in {1, dots, n}$. . To avoid an implicit assumption about numerically close classes being more similar, we model this using a network with $n$ outputs, $z_i, ; i in {1, dots, n}$. We add a final layer to convert each of these into a probability distribution over the associated class being the value of the label: $P(y=i mid x) = a_i = f(z_i)$. . We can think of our model’s final output as a vector that represents a probability distribution over labels. Note that this means we must also make sure to normalise the output values so that they sum to 1. . We use the same approach as in the binary-class case to model $f(z_i)$. We begin with a linear model for the log probability at each output: . zi=log⁡P~(y=i∣x)z_i = log tilde{P}(y=i mid x)zi​=logP~(y=i∣x) . Exponentiating and normalising gives: . P(y=i∣x)=ai=softmax(z)i=ezi∑k=1nezkP(y=i mid x) = a_i = text{softmax}(z)_i = frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}}P(y=i∣x)=ai​=softmax(z)i​=∑k=1n​ezk​ezi​​ . We can think of this $ text{softmax}(z)_i$ function as a “soft” version of the $ arg max$ function; in fact, some suggest that it should more properly be named $ text{softargmax}.$ Softmax is the generalisation of the sigmoid over a vector. It’s derivative is also similar (in fact, for the first case it’s the same). We find this using the quotient rule: . if    i=j:daidzj=ezi∑k=1nezk−eziezj∑k=1nezk=ezi∑k=1nezk(1−ezi∑k=1nezk)=ai(1−ai)if    i≠j:daidzj=0−eziezj∑k=1nezk=−ezi∑k=1nezkezj∑k=1nezk=−aiaj text{if} ; ; i = j: frac{da_i}{dz_j} = frac{e^{z_i} sum_{k=1}^ne^{z_k} - e^{z_i}e^{z_j}}{ sum_{k=1}^ne^{z_k}} = frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} left(1- frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} right) = a_i(1-a_i) text{if} ; ; i ne j: frac{da_i}{dz_j} = frac{0 - e^{z_i}e^{z_j}}{ sum_{k=1}^ne^{z_k}} = - frac{e^{z_i}}{ sum_{k=1}^ne^{z_k}} frac{e^{z_j}}{ sum_{k=1}^ne^{z_k}} = -a_i a_jifi=j:dzj​dai​​=∑k=1n​ezk​ezi​∑k=1n​ezk​−ezi​ezj​​=∑k=1n​ezk​ezi​​(1−∑k=1n​ezk​ezi​​)=ai​(1−ai​)ifi​=j:dzj​dai​​=∑k=1n​ezk​0−ezi​ezj​​=−∑k=1n​ezk​ezi​​∑k=1n​ezk​ezj​​=−ai​aj​ . We can plug the softmax into the NLL to give: . J(θ)=−∑j=1mlog⁡P(y=yj∣x)J(θ)=−∑j=1mlog⁡ayj(we just ignore the other ai≠yj)J(θ)=−∑j=1mlog⁡softmax(zyj)J(θ)=−∑j=1m[zyj−log⁡∑k=1nezk]J( theta) = - sum_{j=1}^m log P(y = y_j mid x) J( theta) = - sum_{j=1}^m log a_{y_j} quad text{(we just ignore the other } a_{i ne y_j} text{)} J( theta) = - sum_{j=1}^m log text{softmax}(z_{y_j}) J( theta) = - sum_{j=1}^m left[z_{y_j} - log sum_{k=1}^n e^{z_k} right] J(θ)=−j=1∑m​logP(y=yj​∣x)J(θ)=−j=1∑m​logayj​​(we just ignore the other ai​=yj​​)J(θ)=−j=1∑m​logsoftmax(zyj​​)J(θ)=−j=1∑m​[zyj​​−logk=1∑n​ezk​] . One point worth noting is that this looks a bit simpler than the binary case - shouldn’t it be at least as complex? The reason for this is due to us having one parameter for each class here, whereas because of the probabilities summing to 1 we only really need $n-1$ parameters. We do this for the binary case, which makes the reasoning a little more complex. . Intuitively, we can understand this cost function as incentivising the correct output to increase and the rest to decrease. The second term also punishes the largest incorrect output the most, which is also desirable. . We can see exactly how learning progresses by calculating the gradient of the NLL. We do this over a single label $y$ for a single $z_i$: . ddziJ(θ)=ddzi(−log⁡ay)ddziJ(θ)=−1ay{ai(1−ai),i=y−aiay,i≠yddziJ(θ)={ai−1,i=yai,i≠yddziJ(θ)=ai−yi(hot)(where y(hot) is the 1-hot encoded label vector) frac{d}{dz_i} J( theta) = frac{d}{dz_i} left(- log a_y right) frac{d}{dz_i} J( theta) = - frac{1}{a_y} begin{cases} a_i(1-a_i), &amp; i = y -a_ia_y, &amp; i ne y end{cases} frac{d}{dz_i} J( theta) = begin{cases} a_i - 1, &amp; i = y a_i, &amp; i ne y end{cases} frac{d}{dz_i} J( theta) = a_i - y^{(hot)}_i quad text{(where } y^{(hot)} text{ is the 1-hot encoded label vector)}dzi​d​J(θ)=dzi​d​(−logay​)dzi​d​J(θ)=−ay​1​{ai​(1−ai​),−ai​ay​,​i=yi​=y​dzi​d​J(θ)={ai​−1,ai​,​i=yi​=y​dzi​d​J(θ)=ai​−yi(hot)​(where y(hot) is the 1-hot encoded label vector) . Fantastic! This is exactly the same as the gradient in the binary-case, but we have it over each item of the output vector instead of the scalar we had before. Everything we deduced for that scenario works just the same here. . We won’t prove it here, but if we create a regression model which outputs the mean of a Gaussian, it turns out that the partial derivatives at for the final layer are also of the form $a_i - y_i$ . It turns out that’s a really simple and desirable gradient for the final layer! . Numerically Stable Softmax . We should instinctively be wary of the exponential and log terms in our softmax function. As we saw in chapter 4, these terms have the potential to drive our values into ranges that can’t be precisely represented by floating-point numbers if we’re not careful. . Specifically, we want to avoid extremely large or extremely negative inputs to the softmax, that will drive the exponentials to infinity or zero respectively. . Fortunately, there is a trick which can help in this regard. It can be easily shown that $ text{softmax}( mathbb{z}) = text{softmax}( mathbb{z} + c)$. From this we can derive our numerically stable variant of softmax: . softmax(z)=softmax(z−max⁡izi) text{softmax}( mathbb{z}) = text{softmax}( mathbb{z} - max_i z_i)softmax(z)=softmax(z−imax​zi​) . This means that our inputs to the function are simply the differences between each input and the largest input. Thus, if the scale of all the inputs becomes very large or negative, the computation will still be stable so long as the relative values are not extremely different across the inputs4. . Wikipedia defines the sigmoid function as a general family of S-shaped curves, and refers to this particular function as the logistic function. &#8617; . | One useful feature of this form is that one case is constant. We will see when we normalise how this translates into a single output controlling the probabilities of both cases, as required for a Bernoulli distribution. &#8617; . | Sigmoid activation combined with MSE has exactly this problem. See how the left extreme of this graph (the derivative of the MSE) tends to 0, whereas in our case it tends to -1. &#8617; . | I believe that the gradient of the softmax should discourage these relative values getting too large, but I’m not sure. If this is the case, then we are protected against numerical instability from all directions. &#8617; . |",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-2/feedforward/neural-network/2020/11/19/feedforward.html",
            "relUrl": "/deep-learning-book/part-2/feedforward/neural-network/2020/11/19/feedforward.html",
            "date": " • Nov 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Deep Learning Book Notes: Probability & Information Theory",
            "content": "Link to chapter: 3 Probability and Information Theory . Common Probability Distributions . Bernoulli Distribution . Support: {$0, 1$} . Parameters: $0 leq p leq 1$ . PMF: . p(x)={1−px=0px=1p(x) = begin{cases} 1-p &amp; quad x=0 p &amp; quad x=1 end{cases}p(x)={1−pp​x=0x=1​ . CDF: . F(x)={0x&lt;01−p0≤x&lt;1px≥1F(x) = begin{cases} 0 &amp; quad x lt 0 1-p &amp; quad 0 leq x lt 1 p &amp; quad x ge 1 end{cases}F(x)=⎩⎪⎪⎨⎪⎪⎧​01−pp​x&lt;00≤x&lt;1x≥1​ . Mean: $p$ . Variance: $p(1-p)$ . Multinoulli / Categorical Distribution . Support: $x in$ {$1, dots, k$} . Parameters: . number of categories: $k &gt; 0$ . | event probabilities: $p_1, dots, p_k quad (p_i gt 0, sum{p_i} = 1)$ . | . PFM: . p(x=i)=pip(x = i) = p_ip(x=i)=pi​ . Normal / Gaussian Distribution . Support: $x in mathbb{R}$ . Parameters: $ mu in mathbb{R}, ; sigma^2 &gt; 0$ . PDF: . p(x)=1σ2πe−12(x−μσ)2p(x) = frac {1}{ sigma { sqrt {2 pi }}}e^{-{ frac {1}{2}} left({ frac {x- mu }{ sigma }} right)^{2}}p(x)=σ2π . ​1​e−21​(σx−μ​)2 . The CDF is more complex and cannot be expressed in terms of elementary functions. . The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. . The following gives an interesting Bayesian interpretation of the normal distribution: . Out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model. . Multivariate normal distribution, PDF: . (2π)−k2det⁡(Σ)−12 e−12(x−μ) ⁣TΣ−1(x−μ){ displaystyle (2 pi )^{-{ frac {k}{2}}} det({ boldsymbol { Sigma }})^{-{ frac {1}{2}}} ,e^{-{ frac {1}{2}}( mathbf {x} -{ boldsymbol { mu }})^{ !{ mathsf {T}}}{ boldsymbol { Sigma }}^{-1}( mathbf {x} -{ boldsymbol { mu }})}}(2π)−2k​det(Σ)−21​e−21​(x−μ)TΣ−1(x−μ) . Exponential Distribution . Support: $x in$ [$0, infty$) . Parameters: $ lambda &gt; 0$ . PDF: . λe−λx lambda e^{- lambda x}λe−λx . CDF: . 1−e−λx1 - e^{- lambda x}1−e−λx . One benefit of using this distribution is that it has a sharp peak at $x = 0$. . Laplace Distribution . Support: $x in mathbb{R}$ . Parameters: . location: $ mu in mathbb{R}$ | scale: $b &gt; 0$ | . PDF: . p(x)=12bexp⁡(−∣x−μ∣b)p(x) = { displaystyle { frac {1}{2b}} exp left(-{ frac {|x- mu |}{b}} right)}p(x)=2b1​exp(−b∣x−μ∣​) . CDF: . F(x)={12exp⁡(x−μb)if x≤μ1−12exp⁡(−x−μb)if x≥μF(x) = { displaystyle { begin{cases}{ frac {1}{2}} exp left({ frac {x- mu }{b}} right)&amp;{ text{if }}x leq mu [8pt]1-{ frac {1}{2}} exp left(-{ frac {x- mu }{b}} right)&amp;{ text{if }}x geq mu end{cases}}}F(x)=⎩⎪⎨⎪⎧​21​exp(bx−μ​)1−21​exp(−bx−μ​)​if x≤μif x≥μ​ . This is similar to the exponential distribution, but it allows us to place the peak anywhere we wish. . It is similar to the normal distribution too, but uses an absolute difference rather than the square. . Dirac Distribution . If we wish to specify that all the mass in a probability distribution clusters around a single point then we can use the Dirac delta function, $ delta(x)$, which is zero-valued everywhere except 0, yet integrates to 1 (this is a special mathematical object called a generalised function). . PDF: $p(x) = delta(x - mu)$ . Empirical Distribution . We can use the Dirac delta function with our training data, $x^{(1)}, dots, x^{(m)}$, to define the following PDF: . p(x)=1m∑i=1mδ(x−x(i))p(x) = frac{1}{m} sum^m_{i=1}{ delta(x - x^{(i)})}p(x)=m1​i=1∑m​δ(x−x(i)) . This concentrates all of the probability mass on the training data. In effect, this distribution represents the distribution that we sample from when we train a model on this dataset. . It is also the PDF that maximises the likelihood of the training data. . Information Theory . Information . The amount of information an event tells us, depends on its likelihood. Frequent events tell us little, while rare events tell us a lot. . Information theory gives us a measure called a nat, that quantifies how much information an event $x$ gives us. We denote this by $I(x)$. . Our requirements for such a function are that it satisfies the following: . An event with probability 1 has $I(x) = 0$ | The less likely an event, the more information it transmits | The information conveyed by independent events should be additive | . We therefore define a nat as follows: . I(x)=−log⁡P(x)I(x) = - log{P(x)}I(x)=−logP(x) . Here we use the natural logarithm. If base 2, is used this measurement is called shannons or bits. . Entropy . Moving to whole probability distributions, we define the expected information in an event sampled from a distribution as the Shannon entropy: . H(x)=Ex∼P[I(x)]H(x) = mathbb{E}_{x sim P}[I(x)]H(x)=Ex∼P​[I(x)] . Distributions that are nearer deterministic have lower entropies, and distributions that are nearer uniform have higher entropies. . When $x$ is continuous this is also known as differential entropy. . KL Divergence . If we want to compare the information in two probability distributions, we use the Kullback-Leibler divergence, which is the expected log probability ratio between the two distributions: . DKL(P∣∣Q)=Ex∼P[log⁡P(x)Q(x)]D_{KL}(P || Q) = mathbb{E}_{x sim P} left[ log frac{P(x)}{Q(x)} right]DKL​(P∣∣Q)=Ex∼P​[logQ(x)P(x)​] . The KL divergence is $0$ when $P$ and $Q$ are the same. . This is sometimes thought of as a measure of “distance” between the two distributions. However, this measure is not symmetric, so does not satisfy the typical requirements of distances. . To visualise the asymmetry, see figure 6.3 in the book. The key point here is that if we wish to minimise the kl: . From the perspective of Q: we want to make sure we have high probability whenever P has high probability (and if P has low probability, Q can be low or high) | From the perspective of P: we want to make sure we have low probability whenever Q has low probability (and if Q has high probability, P can be low or high) | . Cross-Entropy . A similar measure is the cross-entropy, which is defined as: . H(P,Q)=Ex∼P[−log⁡Q(x)]H(P, Q) = mathbb{E}_{x sim P}[- log{Q(x)}]H(P,Q)=Ex∼P​[−logQ(x)] . This measure can be thought of in the following way: . The cross entropy can be interpreted as the number of bits per message needed (on average) to encode events drawn from true distribution p, if using an optimal code for distribution q . Note that the cross entropy can be defined as the Shannon entropy of $P$ plus the KL divergence from $P$ to $Q$: . H(P,Q)=H(P)+DKL(P∣∣Q)H(P, Q) = H(P) + D_{KL}(P || Q)H(P,Q)=H(P)+DKL​(P∣∣Q) . Structured Probabilistic Models . Motivation . The number of parameters in a probability distribution over $n$ random variables is exponential in $n$. Hence, using a single probability distributions over a large number of random variables can be very inefficient. . If we can factorise joint probability distributions into chains of conditional distributions, we can greatly reduce the number of parameters and computational cost. . We call these structured probabilistic models or graphical models. . Directed Models . Given a graph $G$, we define the immediate parents of a node (as defined by the directed edges) as $Pa_G(x_i)$. We can then express the factorisation as follows: . p(x)=∏ip(xi∣PaG(xi))p( mathbf{x}) = prod_i{p(x_i|Pa_G(x_i))}p(x)=i∏​p(xi​∣PaG​(xi​)) . The graph itself effectively encodes a number of (mainly conditional) independence relations between random variables. Specifically, any two nodes are conditionally independent given the values of their parents. This is really what we’re exploiting to gain the efficiency speedup here. . Undirected Models . In undirected models we associate groups of nodes with a factor. . We define a clique $C^{(i)}$ as a set of nodes that are all connected to one-another. . Each clique in the model is then associated with a factor $ phi^{(i)}(C^{(i)})$. Note that these factors are simply non-negative functions; not probability distributions. . To obtain the full joint probability distribution, we then multiply and normalise: p(x)=1Z∏iϕ(i)(C(i))p( mathbf{x}) = frac{1}{Z} prod_i{ phi^{(i)}(C^{(i)})}p(x)=Z1​∏i​ϕ(i)(C(i)) where $Z$ is a normalising constant (i.e. the sum/integral of the probability over all outcomes). .",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-1/probability/information-theory/2020/11/11/prob.html",
            "relUrl": "/deep-learning-book/part-1/probability/information-theory/2020/11/11/prob.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep Learning Book Notes: Linear Algebra",
            "content": "Link to chapter: 2 Linear Algebra . Matrix Multiplication . Key Terms . Span (of a matrix / set of vectors): the set of all vectors obtainable via linear combination . Column / row space: span of a matrix’s columns / rows . Linear independence: no vectors are in the span of any other group of vectors . Invertible matrix: square matrix with linearly independent columns . Motivation . Consider the “basic” matrix-vector multiplication equation: $Ax = b$, where $A$ is a fixed matrix and $x$ is a variable vector to be found. . For a fixed $b$, we care about the possible solutions (i.e. values of $x$): how many are there and what are they? . Considering the case where $b$ is arbitrary (i.e. considering what is true for all $b$) is perhaps more interesting, and can tell us a lot about $A$ and its properties. The key question is: does the equation have a solution for all $b$? . Solutions . In the case of a fixed $b$ the basic equation either has 0, 1 or $ infty$ many solutions. The authors provide a useful way of thinking about this: . To analyze how many solutions the equation has, think of the columns of $A$ as specifying diﬀerent directions we can travel in from the origin … then determine how many ways there are of reaching $b$. . In the case of an arbitrary $b$: . There is &gt;= 1 solution for all $b$ iff $A$ has a set of $m$ linearly independent columns. This is due to the column space of $A$ being all of $ mathbb{R}^{m}$. | A necessary (but not sufficient) condition here is $n gt m$ (at least as many columns as rows), otherwise the column space can’t span $ mathbb{R}^{m}$. | . | There is = 1 solution for all $b$ iff $A$ has exactly $m$ linearly independent columns. A necessary condition is therefore that m = n (i.e. $A$ is square). | If $A$ satisfies this condition we say it is invertible. | Note that a square matrix that is not invertible is called singular or degenerate. | . | . Why is this all useful? Consider… . Inverse Matrices . Think of $A$ as applying a transformation to a vector or matrix. . If the basic equation has one solution (i.e. $A$ is invertible) then this transformation can be reversed. This is often really useful! . This reversal can be expressed as the matrix inverse $A^{-1}$. . In practice computing $A^{-1}$ directly is often avoided as it can be numerically unstable, but this property is still very important. . Norms . Motivation . A norm is a function that gives us some measure of the distance from the origin to a vector or matrix. . Clearly this is a useful concept! . Definition . A norm is any function which satisfies the following three properties (for all $ alpha, x, y$): . point-separating: $f(x) = 0 implies x = 0$ . absolutely scalable: $f( alpha x) = | alpha|f(x)$ . triangle inequality: $f(x + y) &lt;= f(x) + f(y)$ . Vector Norms . The $L^p$ norm of $x$, often denoted by $||x||_p$ , is defined as: . ∣∣x∣∣p=(∑i∣xi∣p)1p||x||_p = left( sum_i |x_i|^p right)^{ frac{1}{p}} quad∣∣x∣∣p​=(i∑​∣xi​∣p)p1​ . where ($p in mathbb{R}, p ge 1$) . . The $L^1$ norm is called the Manhattan norm. . The $L^2$ norm is called the Euclidean norm. This is the standard norm and is commonly referred to without the subscript as simply $||x||$. The squared $L^2$ norm is also used in some contexts, which is simply $x^Tx$. . The $L^ infty$ norm is called the max norm. It is defined as $||x||_ infty = max_i{|x_i|}$. . Matrix Norms . (This is a much more complex field that we only touch on briefly here!) . We consider two analogous matrix norms for the $L^2$ vector norm. . In wider mathematics the spectral norm is often used. It can be useful in ML for analysing (among other things) exploding/vanishing gradients. It is defined as $A$’s largest singular value: $||A|| = sigma_{ max}{(A)}$ . However, most in most ML applications it is assumed the Frobenius norm is used. This is defined as: . ∥∥A∥∥F=∑i,jAi,j2 | |A | |_F = sqrt{ sum_{i,j}{A^2_{i,j}}}∥∥A∥∥F​=i,j∑​Ai,j2​ . ​ . Note that this is equivalent to: $||A||_F = sqrt{Tr(AA^T)}$ . Eigendecomposition . Key Terms . Unit vector: the $L^2$ norm = 1 . Orthogonal vectors: $x^Ty = 0$ . Orthonormal vectors: orthogonal unit vectors . Orthogonal matrix: rows &amp; columns are mutually orthonormal . Motivation . Decomposing matrices can help us learn about a matrix by breaking it down into its constituent parts. This can reveal useful properties about the matrix. . Eigendecomposition decomposes a matrix into eigenvectors and eigenvalues. . These tell us something about the directions and sizes of the transformation created when multiplying by the matrix. . Eigenvectors &amp; Eigenvalues . Vector $v$ and scalar $ lambda$ are a eigenvector-eigenvalue pair for square matrix $A$ iff: . $v neq mathbf{0}$ | $Av = lambda v$ | (Strictly speaking, here $v$ is a right eigenvector. A left eigenvector is such that $v^TA = lambda v^T$. We care primarily about right eigenvectors.) . If $v$ is an eigenvector it follows that any rescaled version of $v$ is also an eigenvector with the same eigenvalue. We typically use a scale such that we have a unit eigenvector. . If $A$ has $n$ independent eigenvectors we can combine them into the columns of a matrix, $V$, such that $AV = V diag( lambda)$. . The eigendecomposition of $A$ is then defined as: $A = V diag( lambda) V^{-1}$. . We are only guaranteed an eigendecomposition if $A$ is symmetric (and real-valued). In this case it is often denoted: . A=QΛQTA = Q Lambda Q^TA=QΛQT . Here the decomposition is guaranteed to be real-valued and $Q$ is orthogonal. . The decomposition may not be unique if two (independent) eigenvectors have the same eigenvalues. . Zero-valued eigenvalues exist iff $A$ is singular. . The Determinant . The determinant, noted $det(A)$, is a scalar that reflects how much multiplying by $A$ alters the volume of an object. . The determinant is calculated by taking the product of $A$’s eigenvalues. . $det(A) = 1 implies$ volume is preserved. . $det(A) = 0 implies$ space is contracted completely in at least one dimension. Thus volume = 0. . Singular Value Decomposition . Motivation . We want a decomposition for all real matrices. SVD provides this. . It is also key to computing the pseudoinverse (TODO), which enables us to find solutions to $Ax = b$ in all cases. . Singular Values &amp; Vectors . The SVD decomposes a matrix into: . A=UDVTA = UDV^TA=UDVT . where: . $U$ is an orthogonal $m times m$ matrix of left-singular vectors (columns) | $V$ is an orthogonal $n times n$ matrix of right-singular vectors (columns) | $D$ is a diagonal $m times n$ matrix of singular values (not necessarily square) | . These matrices are calculated as follows: . $U$ = the (unit norm) eigenvectors of $A^TA$ | $V$ = the (unit norm) eigenvectors of $AA^T$ | $D$ = the square roots of the eigenvalues of either (padded with zeroes if there aren’t enough). | . With the SVD we can calculate the… . Pseudoinverse . Motivation . There are either 0, 1 or $ infty$ solutions to $Ax = b$ . . The inverse, $A^-1$, allows us to find $x$ if there is a singular solution. . However, for the inverse to exist, $A$ must satisfy the properties for being invertible (exactly $m$ linearly independent columns). . The pseudoinverse gives us a method for finding the “best” $x$ for all possible values of $A$ and $b$. . The Moore-Penrose Pseudoinverse . We denote the pseudoinverse as $A^+$ and take our candidate for $x$ as follows: . x′=A+bx^ prime = A^+bx′=A+b . The pseudoinverse is defined as follows: . A+=VD+UTA^+ = VD^+U^TA+=VD+UT . where, . $V$ and $U$ are taken directly from the SVD, although the order here is reversed . | $D^+$ contains the reciprocals of the (non-zero) values in $D$, with a transpose applied to fix the dimensions. . | . When there are $ infty$ valid solutions, the pseudoinverse gives the value of $x^ prime$ with minimal $L^2$ norm. . When there are 0 valid solutions, the pseudoinverse gives the value of $x^ prime$ such that $Ax^ prime - b$ has minimal $L^2$ norm. . Principal Component Analysis . Motivation . We often encounter high dimensional data that can be compressed by projecting it to a lower dimensional space. . We can do this using linear algebra, by multiplying by a matrix (encoding) and multiplying by its inverse (decoding). . We want to find the matrix that minimises information loss when encoding. . Process . Suppose we have data $x^{(1)}, dots , x^{(m)}$ where $x^{(i)} in mathbb{R}^n$. . Encoding/decoding works as follows: . Encoder: $f( cdot)$ | Code: $c = f(x)$ | Decoder: $g( cdot)$ where $x approx g(c)$ | . We first define the decoder: $g(c) = Dc$ where $D in mathbb{R^{n times l}}$. . The columns of $D$ are orthogonal and have unit norm, but not square (so it’s not technically an orthogonal matrix). . The encoder is then defined: $f(x) = D^Tx$. . It can be shown this choice minimises $||x - g(c)||_{2}^{2}$. (i.e. the gradient is zero when $c = D^Tx$) . The entire process defines the PCA reconstruction operation: $r(x) = DD^Tx$. . Choice of Decoding Matrix . The only question that remains is what we should actually choose for the $l$ $n$-dimensional vectors that make up $D$. . It turns out the optimal choice here is the eigenvectors corresponding to the $l$ largest eigenvalues of $X^TX$ (i.e. squared singular values of $X$). . The proof of this relies on the following: . To find $d^{(1)}$ we seek to minimise $|| x^{(1)} - r(x^{(1)}) ||_F^2$. | This can be shown to be equivalent to maximising $Tr left( frac{d^TX^TXd}{w^Tw} right)$ subject to $d^Td = 1$. | A standard result for a positive semidefinite matrix like $X^TX$ is that this fraction’s maximum possible value is equal to the largest eigenvalue of $X^TX$. | To reach this possible value, $d^{(1)}$ is required to equal the corresponding eigenvector. | This can be repeated for the subsequent values of $d$, which then correspond to the eigenvectors of the next smallest eigenvalues. |",
            "url": "https://thecharlieblake.co.uk/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "relUrl": "/deep-learning-book/part-1/linear-algebra/2020/11/08/linear-algebra.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "ImageNet Classification with Deep Convolutional Neural Networks",
            "content": "Introduction . The paper focuses on the domain of visual object recognition. Interpreting high-dimensional pixel data is a difficult task, which requires learning complex relationships. . The authors outline two key components for addressing this: . Large datasets | Prior knowledge | These are addressed as follows: . Using the new ImageNet dataset, which has around $10^7$ images, versus $10^4/10^5$ for previous datasets. | Using the CNN architecture, whose inductive bias effectively encodes prior knowledge. | The paper makes the following contributions: . It demonstrates by far the best ImageNet results to date. | It presents a GPU-optimised implementation for CNNs that is key to scaling to larger datasets. | One key point which the authors perhaps don’t stress enough is as follows: . All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available. . This observation appears key to the enduring success of such methods. . The Dataset . ImageNet contains 15 million images of variable size, in roughly 22,000 categories. The images were taken from the web, and were labelled using Mechanical Turk. . The competition in question is the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), which has run since 2010. ILSVRC uses a subset of ImageNet with 1.2 million training images across 1000 categories. . On ImageNet the top-1 and top-5 error rates are typically reported, where ‘top-n’ denotes that the correct label is in the ‘n’ labels which a model gives the highest probability. . The only pre-processing is as follows: . Images are scaled to 256x256 | Each pixel is normalised according to the ‘mean activity’ across the dataset1. | Implementation . Architecture . The overall architecture contains 5 convolutional layers, followed by 3 fully-connected layers, and finally a softmax over the 1000 outputs. . The loss/objective is multinomial logistic regression. . Key Features . The authors describe their key implementation features from most to least important: . ReLU Nonlinearity . In 2012 the standard nonlinearity was $tanh$, but based on (Nair &amp; Hinton, 2010) they use $ReLU$ instead. . They demonstrate a 6x speedup on CIFAR-10 (a smaller dataset) when comparing the two. . Training on Multiple GPUs . They spread the neurons across two GPUs. Each is able to read/write to the other’s memory directly (i.e. without going through the main memory). . One trick they employ to make this work is to only have the GPUs communicate across certain layers. E.g. layer 2 is fully-connected to layer 3 across the two GPUs, but layer 3 is only fully-connected to layer 4 within the same GPU. This pattern is tuned as a hyperparameter (see the paper for an image detailing the final connectivity scheme, plus the layer/kernel dimensions). . This scheme reduces the error rates by (1.7%, 1.2%) compared with the single-GPU net. . Local Response Normalisation . One desirable property of ReLU is that is doesn’t require input normalisation to prevent saturation. However, the authors still find an advantage to be gained by using local normalisation. . Their scheme localises each activation in the network by a factor based on the mean squared activations of adjacent activations. This is not done between all layers, but only the first two. . This scheme reduces the error rates by (1.4%, 1.2%). . Overlapping Pooling . Previous CNN architectures did not overlap pooling layers, but here they do. This scheme reduces the error rates by (0.4%, 0.3%). . Reducing Overfitting . The model has 60 million parameters. The following techniques are designed to stop this large number of parameters from overfitting. . Data Augmentation . Augmentation is effectively computationally ‘free’, as it done on the CPU while waiting for the previous batch to train on the GPU. The following techniques are used: . Translations and horizontal reflections. | Altering pixel intensities (using a fairly complex PCA-based scheme to change the intensity and colour of the overall image). | Dropout . Ensemble learning is known to be a very successful way of improving model performance. The authors build on this by using the dropout technique introduced by (Hinton et al., 2012), which has a similar effect but using a single model. . For each batch, the output of each neuron is set to 0 with a probability of 0.5. The effectiveness of this is justified in the following way: . This technique reduces complex co-adaptations of neurons,since a neuron cannot rely on the presence of particular other neurons. . Dropout is applied to the first two fully-connected layers. . Weight Decay . The weight decay parameter is a small value: 0.0005. Nevertheless, this was found to be ‘important for the model to learn’. . Results . On ILSVRC-2010 the previous best result was (45.7%, 25.7%). The CNN model achieves (37.5%, 17.0%). . On ILSVRC-2012 the second-best result was (. , 26.2%). The CNN model achieves (. , 18.2%) **, and using an ensemble of 7 CNNs where 2 are pre-trained on ILSVRC-2011 achieves **(. , 15.3%). . Discussion . The authors highlight the following points of note: . CNNs are a highly effective model architecture. | Their depth is key here, as removing even a single layer damages performance. | Only supervised learning is necessary to achieve these results. | They further speculate that: . Unsupervised pre-training could help significantly, especially in the case where the dataset stays the same size but the number of model parameters increases. | They are still several orders of magnitude away from the human visual system. | These networks could be especially powerful for video sequences, where the temporal structure provides further useful information. | My Thoughts . Why It Works . A few points seem notable in terms of why these methods have proved so effective. I will give these in order of significance: . These methods are designed to scale with compute and data: this presumably paved the way for CNNs to make such progress in computer vision. | The size of ImageNet: this dataset, which was relatively new at this point, is several orders of magnitude larger than previous datasets. This gave huge scope for a big breakthrough. | Hardware made this possible: training such a large model was only really possible because of GPUs. They again rode this wave to great effect. | Some neat tricks: ReLU, dropout, data augmentation, normalisation and weight decay have become central tools. Each of these knocked a small chunk off the error rate. Such tricks are very important and easily overlooked. | Big hyperparameter search: although exact details aren’t reported, it sounds like the authors were able to throw a lot of compute at getting tuning many different aspects of the model, even down to the GPU communication. It should not be underestimated the extent to which throwing resources at this problem can yield substantial performance increases. | The Paper Itself . The paper is well-structured, to-the-point and clear. . One criticism I have is with a lack of discussion of the methods they are comparing against though, which would have helped contextualise/justify their approach. I also would like more assurance that they are making a fair comparison with other methods. I’m not certain, for instance how the methods they compare against would fare if given an equal amount of compute (although given the success of CNNs here and subsequently, I would almost certainly expect them to do better under such conditions). . I would also have loved to see a proper table of ablations in an appendix (plus details of hyperparameter search). They do have some great ablation details in the body of the paper, but it would be good to see them all done thoroughly in one place. . Thoughts for Further Reading . In a rough order of priority: . The original paper on dropout by (Hinton et al., 2012). | The original paper on ReLU by (Nair &amp; Hinton, 2010). | Subsequent papers which improve on ILSVRC. | Other subsequent key papers in CV. | As suggested in the discussion, any work done using CNNs for video data. | It’s not clear to me what is meant by ‘mean activity’ here. Does activity mean pixel value? And is this per-pixel or across all of them? &#8617; . |",
            "url": "https://thecharlieblake.co.uk/paper-notes/krizhevsky2017imagenet.html",
            "relUrl": "/paper-notes/krizhevsky2017imagenet.html",
            "date": " • Nov 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "As of October 2020, I’ve just finished my MSc in Computer Science at Oxford University, and am looking to work in Data Science / Machine Learning Engineering. I did my undergraduate degree at the University of St Andrews. My interests include Deep Learning, Reinforcement Learning, Graph Neural Networks and Solitaire card games. .",
          "url": "https://thecharlieblake.co.uk/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://thecharlieblake.co.uk/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}